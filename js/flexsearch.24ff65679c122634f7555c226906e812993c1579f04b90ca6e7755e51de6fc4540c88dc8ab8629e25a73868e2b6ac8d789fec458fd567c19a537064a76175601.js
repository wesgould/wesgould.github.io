(()=>{var ne=Object.create;var te=Object.defineProperty;var se=Object.getOwnPropertyDescriptor;var ae=Object.getOwnPropertyNames;var re=Object.getPrototypeOf,ue=Object.prototype.hasOwnProperty;var le=(e,o)=>()=>(o||e((o={exports:{}}).exports,o),o.exports);var he=(e,o,i,n)=>{if(o&&typeof o=="object"||typeof o=="function")for(let s of ae(o))!ue.call(e,s)&&s!==i&&te(e,s,{get:()=>o[s],enumerable:!(n=se(o,s))||n.enumerable});return e};var de=(e,o,i)=>(i=e!=null?ne(re(e)):{},he(o||!e||!e.__esModule?te(i,"default",{value:e,enumerable:!0}):i,e));var oe=le((exports,module)=>{(function _f(self){"use strict";try{module&&(self=module)}catch(e){}self._factory=_f;var t;function u(e){return typeof e!="undefined"?e:!0}function aa(e){let o=Array(e);for(let i=0;i<e;i++)o[i]=v();return o}function v(){return Object.create(null)}function ba(e,o){return o.length-e.length}function x(e){return typeof e=="string"}function C(e){return typeof e=="object"}function D(e){return typeof e=="function"}function ca(e,o){var i=da;if(e&&(o&&(e=E(e,o)),this.H&&(e=E(e,this.H)),this.J&&1<e.length&&(e=E(e,this.J)),i||i==="")){if(e=e.split(i),this.filter){o=this.filter,i=e.length;let n=[];for(let s=0,r=0;s<i;s++){let l=e[s];l&&!o[l]&&(n[r++]=l)}e=n}return e}return e}let da=/[\p{Z}\p{S}\p{P}\p{C}]+/u,ea=/[\u0300-\u036f]/g;function fa(e,o){let i=Object.keys(e),n=i.length,s=[],r="",l=0;for(let h=0,p,g;h<n;h++)p=i[h],(g=e[p])?(s[l++]=F(o?"(?!\\b)"+p+"(\\b|_)":p),s[l++]=g):r+=(r?"|":"")+p;return r&&(s[l++]=F(o?"(?!\\b)("+r+")(\\b|_)":"("+r+")"),s[l]=""),s}function E(e,o){for(let i=0,n=o.length;i<n&&(e=e.replace(o[i],o[i+1]),e);i+=2);return e}function F(e){return new RegExp(e,"g")}function ha(e){let o="",i="";for(let n=0,s=e.length,r;n<s;n++)(r=e[n])!==i&&(o+=i=r);return o}var ja={encode:ia,F:!1,G:""};function ia(e){return ca.call(this,(""+e).toLowerCase(),!1)}let ka={},G={};function la(e){I(e,"add"),I(e,"append"),I(e,"search"),I(e,"update"),I(e,"remove")}function I(e,o){e[o+"Async"]=function(){let i=this,n=arguments;var s=n[n.length-1];let r;return D(s)&&(r=s,delete n[n.length-1]),s=new Promise(function(l){setTimeout(function(){i.async=!0;let h=i[o].apply(i,n);i.async=!1,l(h)})}),r?(s.then(r),this):s}}function ma(e,o,i,n){let s=e.length,r=[],l,h,p=0;n&&(n=[]);for(let g=s-1;0<=g;g--){let f=e[g],q=f.length,w=v(),k=!l;for(let m=0;m<q;m++){let y=f[m],_=y.length;if(_)for(let B=0,R,A;B<_;B++)if(A=y[B],l){if(l[A]){if(!g){if(i)i--;else if(r[p++]=A,p===o)return r}(g||n)&&(w[A]=1),k=!0}if(n&&(R=(h[A]||0)+1,h[A]=R,R<s)){let H=n[R-2]||(n[R-2]=[]);H[H.length]=A}}else w[A]=1}if(n)l||(h=w);else if(!k)return[];l=w}if(n)for(let g=n.length-1,f,q;0<=g;g--){f=n[g],q=f.length;for(let w=0,k;w<q;w++)if(k=f[w],!l[k]){if(i)i--;else if(r[p++]=k,p===o)return r;l[k]=1}}return r}function na(e,o){let i=v(),n=v(),s=[];for(let r=0;r<e.length;r++)i[e[r]]=1;for(let r=0,l;r<o.length;r++){l=o[r];for(let h=0,p;h<l.length;h++)p=l[h],i[p]&&!n[p]&&(n[p]=1,s[s.length]=p)}return s}function J(e){this.l=e!==!0&&e,this.cache=v(),this.h=[]}function oa(e,o,i){C(e)&&(e=e.query);let n=this.cache.get(e);return n||(n=this.search(e,o,i),this.cache.set(e,n)),n}J.prototype.set=function(e,o){if(!this.cache[e]){var i=this.h.length;for(i===this.l?delete this.cache[this.h[i-1]]:i++,--i;0<i;i--)this.h[i]=this.h[i-1];this.h[0]=e}this.cache[e]=o},J.prototype.get=function(e){let o=this.cache[e];if(this.l&&o&&(e=this.h.indexOf(e))){let i=this.h[e-1];this.h[e-1]=this.h[e],this.h[e]=i}return o};let qa={memory:{charset:"latin:extra",D:3,B:4,m:!1},performance:{D:3,B:3,s:!1,context:{depth:2,D:1}},match:{charset:"latin:extra",G:"reverse"},score:{charset:"latin:advanced",D:20,B:3,context:{depth:3,D:9}},default:{}};function ra(e,o,i,n,s,r,l){setTimeout(function(){let h=e(i?i+"."+n:n,JSON.stringify(l));h&&h.then?h.then(function(){o.export(e,o,i,s,r+1)}):o.export(e,o,i,s,r+1)})}function K(e,o){if(!(this instanceof K))return new K(e);var i;if(e){x(e)?e=qa[e]:(i=e.preset)&&(e=Object.assign({},i[i],e)),i=e.charset;var n=e.lang;x(i)&&(i.indexOf(":")===-1&&(i+=":default"),i=G[i]),x(n)&&(n=ka[n])}else e={};let s,r,l=e.context||{};if(this.encode=e.encode||i&&i.encode||ia,this.register=o||v(),this.D=s=e.resolution||9,this.G=o=i&&i.G||e.tokenize||"strict",this.depth=o==="strict"&&l.depth,this.l=u(l.bidirectional),this.s=r=u(e.optimize),this.m=u(e.fastupdate),this.B=e.minlength||1,this.C=e.boost,this.map=r?aa(s):v(),this.A=s=l.resolution||1,this.h=r?aa(s):v(),this.F=i&&i.F||e.rtl,this.H=(o=e.matcher||n&&n.H)&&fa(o,!1),this.J=(o=e.stemmer||n&&n.J)&&fa(o,!0),i=o=e.filter||n&&n.filter){i=o,n=v();for(let h=0,p=i.length;h<p;h++)n[i[h]]=1;i=n}this.filter=i,this.cache=(o=e.cache)&&new J(o)}t=K.prototype,t.append=function(e,o){return this.add(e,o,!0)},t.add=function(e,o,i,n){if(o&&(e||e===0)){if(!n&&!i&&this.register[e])return this.update(e,o);if(o=this.encode(o),n=o.length){let g=v(),f=v(),q=this.depth,w=this.D;for(let k=0;k<n;k++){let m=o[this.F?n-1-k:k];var s=m.length;if(m&&s>=this.B&&(q||!f[m])){var r=L(w,n,k),l="";switch(this.G){case"full":if(2<s){for(r=0;r<s;r++)for(var h=s;h>r;h--)if(h-r>=this.B){var p=L(w,n,k,s,r);l=m.substring(r,h),M(this,f,l,p,e,i)}break}case"reverse":if(1<s){for(h=s-1;0<h;h--)l=m[h]+l,l.length>=this.B&&M(this,f,l,L(w,n,k,s,h),e,i);l=""}case"forward":if(1<s){for(h=0;h<s;h++)l+=m[h],l.length>=this.B&&M(this,f,l,r,e,i);break}default:if(this.C&&(r=Math.min(r/this.C(o,m,k)|0,w-1)),M(this,f,m,r,e,i),q&&1<n&&k<n-1){for(s=v(),l=this.A,r=m,h=Math.min(q+1,n-k),s[r]=1,p=1;p<h;p++)if((m=o[this.F?n-1-k-p:k+p])&&m.length>=this.B&&!s[m]){s[m]=1;let y=this.l&&m>r;M(this,g,y?r:m,L(l+(n/2>l?0:1),n,k,h-1,p-1),e,i,y?m:r)}}}}}this.m||(this.register[e]=1)}}return this};function L(e,o,i,n,s){return i&&1<e?o+(n||0)<=e?i+(s||0):(e-1)/(o+(n||0))*(i+(s||0))+1|0:0}function M(e,o,i,n,s,r,l){let h=l?e.h:e.map;(!o[i]||l&&!o[i][l])&&(e.s&&(h=h[n]),l?(o=o[i]||(o[i]=v()),o[l]=1,h=h[l]||(h[l]=v())):o[i]=1,h=h[i]||(h[i]=[]),e.s||(h=h[n]||(h[n]=[])),r&&h.includes(s)||(h[h.length]=s,e.m&&(e=e.register[s]||(e.register[s]=[]),e[e.length]=h)))}t.search=function(e,o,i){i||(!o&&C(e)?(i=e,e=i.query):C(o)&&(i=o));let n=[],s,r,l=0;if(i){e=i.query||e,o=i.limit,l=i.offset||0;var h=i.context;r=i.suggest}if(e&&(e=this.encode(""+e),s=e.length,1<s)){i=v();var p=[];for(let f=0,q=0,w;f<s;f++)if((w=e[f])&&w.length>=this.B&&!i[w])if(this.s||r||this.map[w])p[q++]=w,i[w]=1;else return n;e=p,s=e.length}if(!s)return n;o||(o=100),h=this.depth&&1<s&&h!==!1,i=0;let g;h?(g=e[0],i=1):1<s&&e.sort(ba);for(let f,q;i<s;i++){if(q=e[i],h?(f=sa(this,n,r,o,l,s===2,q,g),r&&f===!1&&n.length||(g=q)):f=sa(this,n,r,o,l,s===1,q),f)return f;if(r&&i===s-1){if(p=n.length,!p){if(h){h=0,i=-1;continue}return n}if(p===1)return ta(n[0],o,l)}}return ma(n,o,l,r)};function sa(e,o,i,n,s,r,l,h){let p=[],g=h?e.h:e.map;if(e.s||(g=ua(g,l,h,e.l)),g){let f=0,q=Math.min(g.length,h?e.A:e.D);for(let w=0,k=0,m,y;w<q&&!((m=g[w])&&(e.s&&(m=ua(m,l,h,e.l)),s&&m&&r&&(y=m.length,y<=s?(s-=y,m=null):(m=m.slice(s),s=0)),m&&(p[f++]=m,r&&(k+=m.length,k>=n))));w++);if(f){if(r)return ta(p,n,0);o[o.length]=p;return}}return!i&&p}function ta(e,o,i){return e=e.length===1?e[0]:[].concat.apply([],e),i||e.length>o?e.slice(i,i+o):e}function ua(e,o,i,n){return i?(n=n&&o>i,e=(e=e[n?o:i])&&e[n?i:o]):e=e[o],e}t.contain=function(e){return!!this.register[e]},t.update=function(e,o){return this.remove(e).add(e,o)},t.remove=function(e,o){let i=this.register[e];if(i){if(this.m)for(let n=0,s;n<i.length;n++)s=i[n],s.splice(s.indexOf(e),1);else N(this.map,e,this.D,this.s),this.depth&&N(this.h,e,this.A,this.s);if(o||delete this.register[e],this.cache){o=this.cache;for(let n=0,s,r;n<o.h.length;n++)r=o.h[n],s=o.cache[r],s.includes(e)&&(o.h.splice(n--,1),delete o.cache[r])}}return this};function N(e,o,i,n,s){let r=0;if(e.constructor===Array)if(s)o=e.indexOf(o),o!==-1?1<e.length&&(e.splice(o,1),r++):r++;else{s=Math.min(e.length,i);for(let l=0,h;l<s;l++)(h=e[l])&&(r=N(h,o,i,n,s),n||r||delete e[l])}else for(let l in e)(r=N(e[l],o,i,n,s))||delete e[l];return r}t.searchCache=oa,t.export=function(e,o,i,n,s){let r,l;switch(s||(s=0)){case 0:if(r="reg",this.m){l=v();for(let h in this.register)l[h]=1}else l=this.register;break;case 1:r="cfg",l={doc:0,opt:this.s?1:0};break;case 2:r="map",l=this.map;break;case 3:r="ctx",l=this.h;break;default:return}return ra(e,o||this,i,r,n,s,l),!0},t.import=function(e,o){if(o)switch(x(o)&&(o=JSON.parse(o)),e){case"cfg":this.s=!!o.opt;break;case"reg":this.m=!1,this.register=o;break;case"map":this.map=o;break;case"ctx":this.h=o}},la(K.prototype);function va(e){e=e.data;var o=self._index;let i=e.args;var n=e.task;switch(n){case"init":n=e.options||{},e=e.factory,o=n.encode,n.cache=!1,o&&o.indexOf("function")===0&&(n.encode=Function("return "+o)()),e?(Function("return "+e)()(self),self._index=new self.FlexSearch.Index(n),delete self.FlexSearch):self._index=new K(n);break;default:e=e.id,o=o[n].apply(o,i),postMessage(n==="search"?{id:e,msg:o}:{id:e})}}let wa=0;function O(e){if(!(this instanceof O))return new O(e);var o;e?D(o=e.encode)&&(e.encode=o.toString()):e={},(o=(self||window)._factory)&&(o=o.toString());let i=typeof window=="undefined"&&self.exports,n=this;this.o=xa(o,i,e.worker),this.h=v(),this.o&&(i?this.o.on("message",function(s){n.h[s.id](s.msg),delete n.h[s.id]}):this.o.onmessage=function(s){s=s.data,n.h[s.id](s.msg),delete n.h[s.id]},this.o.postMessage({task:"init",factory:o,options:e}))}P("add"),P("append"),P("search"),P("update"),P("remove");function P(e){O.prototype[e]=O.prototype[e+"Async"]=function(){let o=this,i=[].slice.call(arguments);var n=i[i.length-1];let s;return D(n)&&(s=n,i.splice(i.length-1,1)),n=new Promise(function(r){setTimeout(function(){o.h[++wa]=r,o.o.postMessage({task:e,id:wa,args:i})})}),s?(n.then(s),this):n}}function xa(a,b,c){let d;try{d=b?eval('new (require("worker_threads")["Worker"])("../dist/node/node.js")'):a?new Worker(URL.createObjectURL(new Blob(["onmessage="+va.toString()],{type:"text/javascript"}))):new Worker(x(c)?c:"worker/worker.js",{type:"module"})}catch(e){}return d}function Q(e){if(!(this instanceof Q))return new Q(e);var o=e.document||e.doc||e,i;this.K=[],this.h=[],this.A=[],this.register=v(),this.key=(i=o.key||o.id)&&S(i,this.A)||"id",this.m=u(e.fastupdate),this.C=(i=o.store)&&i!==!0&&[],this.store=i&&v(),this.I=(i=o.tag)&&S(i,this.A),this.l=i&&v(),this.cache=(i=e.cache)&&new J(i),e.cache=!1,this.o=e.worker,this.async=!1,i=v();let n=o.index||o.field||o;x(n)&&(n=[n]);for(let s=0,r,l;s<n.length;s++)r=n[s],x(r)||(l=r,r=r.field),l=C(l)?Object.assign({},e,l):e,this.o&&(i[r]=new O(l),i[r].o||(this.o=!1)),this.o||(i[r]=new K(l,this.register)),this.K[s]=S(r,this.A),this.h[s]=r;if(this.C)for(e=o.store,x(e)&&(e=[e]),o=0;o<e.length;o++)this.C[o]=S(e[o],this.A);this.index=i}function S(e,o){let i=e.split(":"),n=0;for(let s=0;s<i.length;s++)e=i[s],0<=e.indexOf("[]")&&(e=e.substring(0,e.length-2))&&(o[n]=!0),e&&(i[n++]=e);return n<i.length&&(i.length=n),1<n?i:i[0]}function T(e,o){if(x(o))e=e[o];else for(let i=0;e&&i<o.length;i++)e=e[o[i]];return e}function U(e,o,i,n,s){if(e=e[s],n===i.length-1)o[s]=e;else if(e)if(e.constructor===Array)for(o=o[s]=Array(e.length),s=0;s<e.length;s++)U(e,o,i,n,s);else o=o[s]||(o[s]=v()),s=i[++n],U(e,o,i,n,s)}function V(e,o,i,n,s,r,l,h){if(e=e[l])if(n===o.length-1){if(e.constructor===Array){if(i[n]){for(o=0;o<e.length;o++)s.add(r,e[o],!0,!0);return}e=e.join(" ")}s.add(r,e,h,!0)}else if(e.constructor===Array)for(l=0;l<e.length;l++)V(e,o,i,n,s,r,l,h);else l=o[++n],V(e,o,i,n,s,r,l,h)}t=Q.prototype,t.add=function(e,o,i){if(C(e)&&(o=e,e=T(o,this.key)),o&&(e||e===0)){if(!i&&this.register[e])return this.update(e,o);for(let n=0,s,r;n<this.h.length;n++)r=this.h[n],s=this.K[n],x(s)&&(s=[s]),V(o,s,this.A,0,this.index[r],e,s[0],i);if(this.I){let n=T(o,this.I),s=v();x(n)&&(n=[n]);for(let r=0,l,h;r<n.length;r++)if(l=n[r],!s[l]&&(s[l]=1,h=this.l[l]||(this.l[l]=[]),!i||!h.includes(e))&&(h[h.length]=e,this.m)){let p=this.register[e]||(this.register[e]=[]);p[p.length]=h}}if(this.store&&(!i||!this.store[e])){let n;if(this.C){n=v();for(let s=0,r;s<this.C.length;s++)r=this.C[s],x(r)?n[r]=o[r]:U(o,n,r,0,r[0])}this.store[e]=n||o}}return this},t.append=function(e,o){return this.add(e,o,!0)},t.update=function(e,o){return this.remove(e).add(e,o)},t.remove=function(e){if(C(e)&&(e=T(e,this.key)),this.register[e]){for(var o=0;o<this.h.length&&(this.index[this.h[o]].remove(e,!this.o),!this.m);o++);if(this.I&&!this.m)for(let i in this.l){o=this.l[i];let n=o.indexOf(e);n!==-1&&(1<o.length?o.splice(n,1):delete this.l[i])}this.store&&delete this.store[e],delete this.register[e]}return this},t.search=function(e,o,i,n){i||(!o&&C(e)?(i=e,e=""):C(o)&&(i=o,o=0));let s=[],r=[],l,h,p,g,f,q,w=0;if(i)if(i.constructor===Array)p=i,i=null;else{if(e=i.query||e,p=(l=i.pluck)||i.index||i.field,g=i.tag,h=this.store&&i.enrich,f=i.bool==="and",o=i.limit||o||100,q=i.offset||0,g&&(x(g)&&(g=[g]),!e)){for(let m=0,y;m<g.length;m++)(y=ya.call(this,g[m],o,q,h))&&(s[s.length]=y,w++);return w?s:[]}x(p)&&(p=[p])}p||(p=this.h),f=f&&(1<p.length||g&&1<g.length);let k=!n&&(this.o||this.async)&&[];for(let m=0,y,_,B;m<p.length;m++){let R;if(_=p[m],x(_)||(R=_,_=R.field,e=R.query||e,o=R.limit||o),k)k[m]=this.index[_].searchAsync(e,o,R||i);else{if(n?y=n[m]:y=this.index[_].search(e,o,R||i),B=y&&y.length,g&&B){let A=[],H=0;f&&(A[0]=[y]);for(let $=0,ee,j;$<g.length;$++)ee=g[$],(B=(j=this.l[ee])&&j.length)&&(H++,A[A.length]=f?[j]:j);H&&(y=f?ma(A,o||100,q||0):na(y,A),B=y.length)}if(B)r[w]=_,s[w++]=y;else if(f)return[]}}if(k){let m=this;return new Promise(function(y){Promise.all(k).then(function(_){y(m.search(e,o,i,_))})})}if(!w)return[];if(l&&(!h||!this.store))return s[0];for(let m=0,y;m<r.length;m++){if(y=s[m],y.length&&h&&(y=za.call(this,y)),l)return y;s[m]={field:r[m],result:y}}return s};function ya(e,o,i,n){let s=this.l[e],r=s&&s.length-i;if(r&&0<r)return(r>o||i)&&(s=s.slice(i,i+o)),n&&(s=za.call(this,s)),{tag:e,result:s}}function za(e){let o=Array(e.length);for(let i=0,n;i<e.length;i++)n=e[i],o[i]={id:n,doc:this.store[n]};return o}t.contain=function(e){return!!this.register[e]},t.get=function(e){return this.store[e]},t.set=function(e,o){return this.store[e]=o,this},t.searchCache=oa,t.export=function(e,o,i,n,s){if(s||(s=0),n||(n=0),n<this.h.length){let r=this.h[n],l=this.index[r];o=this,setTimeout(function(){l.export(e,o,s?r:"",n,s++)||(n++,s=1,o.export(e,o,r,n,s))})}else{let r,l;switch(s){case 1:r="tag",l=this.l;break;case 2:r="store",l=this.store;break;default:return}ra(e,this,i,r,n,s,l)}},t.import=function(e,o){if(o)switch(x(o)&&(o=JSON.parse(o)),e){case"tag":this.l=o;break;case"reg":this.m=!1,this.register=o;for(let n=0,s;n<this.h.length;n++)s=this.index[this.h[n]],s.register=o,s.m=!1;break;case"store":this.store=o;break;default:e=e.split(".");let i=e[0];e=e[1],i&&e&&this.index[i].import(e,o)}},la(Q.prototype);var Ba={encode:Aa,F:!1,G:""};let Ca=[F("[\xE0\xE1\xE2\xE3\xE4\xE5]"),"a",F("[\xE8\xE9\xEA\xEB]"),"e",F("[\xEC\xED\xEE\xEF]"),"i",F("[\xF2\xF3\xF4\xF5\xF6\u0151]"),"o",F("[\xF9\xFA\xFB\xFC\u0171]"),"u",F("[\xFD\u0177\xFF]"),"y",F("\xF1"),"n",F("[\xE7c]"),"k",F("\xDF"),"s",F(" & ")," and "];function Aa(e){var o=e=""+e;return o.normalize&&(o=o.normalize("NFD").replace(ea,"")),ca.call(this,o.toLowerCase(),!e.normalize&&Ca)}var Ea={encode:Da,F:!1,G:"strict"};let Fa=/[^a-z0-9]+/,Ga={b:"p",v:"f",w:"f",z:"s",x:"s",\u00DF:"s",d:"t",n:"m",c:"k",g:"k",j:"k",q:"k",i:"e",y:"e",u:"o"};function Da(e){e=Aa.call(this,e).join(" ");let o=[];if(e){let i=e.split(Fa),n=i.length;for(let s=0,r,l=0;s<n;s++)if((e=i[s])&&(!this.filter||!this.filter[e])){r=e[0];let h=Ga[r]||r,p=h;for(let g=1;g<e.length;g++){r=e[g];let f=Ga[r]||r;f&&f!==p&&(h+=f,p=f)}o[l++]=h}}return o}var Ia={encode:Ha,F:!1,G:""};let Ja=[F("ae"),"a",F("oe"),"o",F("sh"),"s",F("th"),"t",F("ph"),"f",F("pf"),"f",F("(?![aeo])h(?![aeo])"),"",F("(?!^[aeo])h(?!^[aeo])"),""];function Ha(e,o){return e&&(e=Da.call(this,e).join(" "),2<e.length&&(e=E(e,Ja)),o||(1<e.length&&(e=ha(e)),e&&(e=e.split(" ")))),e||[]}var La={encode:Ka,F:!1,G:""};let Ma=F("(?!\\b)[aeo]");function Ka(e){return e&&(e=Ha.call(this,e,!0),1<e.length&&(e=e.replace(Ma,"")),1<e.length&&(e=ha(e)),e&&(e=e.split(" "))),e||[]}G["latin:default"]=ja,G["latin:simple"]=Ba,G["latin:balance"]=Ea,G["latin:advanced"]=Ia,G["latin:extra"]=La;let W=self,Y,Z={Index:K,Document:Q,Worker:O,registerCharset:function(e,o){G[e]=o},registerLanguage:function(e,o){ka[e]=o}};(Y=W.define)&&Y.amd?Y([],function(){return Z}):W.exports?W.exports=Z:W.FlexSearch=Z})(exports)});var ie=de(oe());var{Document:ce}=ie.default,X=document.getElementById("search__text"),z=document.getElementById("search__suggestions");X!==null&&document.addEventListener("keydown",e=>{e.ctrlKey&&e.key==="/"?(e.preventDefault(),X.focus()):e.key==="Escape"&&(X.blur(),z.classList.add("search__suggestions--hidden"))});document.addEventListener("click",e=>{z.contains(e.target)||z.classList.add("search__suggestions--hidden")});document.addEventListener("keydown",e=>{if(z.classList.contains("search__suggestions--hidden"))return;let i=[...z.querySelectorAll("a")];if(i.length===0)return;let n=i.indexOf(document.activeElement);if(e.key==="ArrowDown"){e.preventDefault();let s=n+1<i.length?n+1:n;i[s].focus()}else e.key==="ArrowUp"&&(e.preventDefault(),nextIndex=n>0?n-1:0,i[nextIndex].focus())});(function(){let e=new ce({tokenize:"forward",cache:100,document:{id:"id",store:["href","title","description"],index:["title","description","content"]}});e.add({id:0,href:"/projects/voxdex-ai-transcription/",title:"AI-Powered Podcast Transcription System",description:`AI Auto Transcription System# Introduction# About two years ago, right as the LLM craze was really popping off, I stumbled on WhisperAI&ndash;a tool released by OpenAI that let you transcribe audio (voice-to-text). I actually uncovered it when I was looking at a way to dictate some of my personal notes in Neovim (vimwiki). I found nerd-dictation [https://github.com/ideasman42/nerd-dictation] which was super amazing, but didn&rsquo;t work on my macbook and died on me as linux began the march away from x11 to wayland. Nerd-dictation required some of the security&hellip;&ldquo;openness&rdquo; that X11 afforded. I switched to wayland to get something working on my Pop_OS! machine, and when I did that, I lost my ability to use nerd-dictation.
`,content:`AI Auto Transcription System# Introduction# About two years ago, right as the LLM craze was really popping off, I stumbled on WhisperAI&ndash;a tool released by OpenAI that let you transcribe audio (voice-to-text). I actually uncovered it when I was looking at a way to dictate some of my personal notes in Neovim (vimwiki). I found nerd-dictation [https://github.com/ideasman42/nerd-dictation] which was super amazing, but didn&rsquo;t work on my macbook and died on me as linux began the march away from x11 to wayland. Nerd-dictation required some of the security&hellip;&ldquo;openness&rdquo; that X11 afforded. I switched to wayland to get something working on my Pop_OS! machine, and when I did that, I lost my ability to use nerd-dictation.
That&rsquo;s was around the time I heard about WhisperCPP on a TWiT podcast. It&rsquo;s a fantastic project by Georgi Gerganov, and you can find information on it here https://github.com/ggerganov/whisper.cpp. As an aside, Georgi Gerganov is the same guy of gguf fame. A leader in AI development/AI tool development / open source. He&rsquo;s ported the OpenAI Whisper code to C++ and even added support for the Core ML cores on Apple Silicon. His implementation is the fastest I&rsquo;ve seen so far (especially when taking advantage of the ML cores on the M-series chips). When I started playing around with it I realized it could do much more than dictation. Which gave me the idea for transcribing the podcasts I listen to.
Why?# As an avid listener of the TWiT (This Week in Tech) network for the last 18 years 20 years, I often find myself trying to remember what someone said, how something was phrased, or &ldquo;what was it that they said about privacy regulation?&rdquo; Obviously, it&rsquo;s pretty tough to remember the episode (or even the right show), and it&rsquo;s impractical to re-listen to thousands of hours of audio. So I had the idea of transcribing the audio and video so that it&rsquo;s easily searchable text.
I want to say I was on paternity leave when I wrote the first very kludgey version of this tool. It consisted of a couple of poorly written python scripts. Some of which I copy and pasted from stackoverflow and various forums on the internet. It was glued together with bash scripts and cron jobs&hellip;and as you can imagine it was incredibly fragile. Every time I rebooted the machine I had to go relaunch things. I tried to automate this but it was so finicky that I often needed to ssh into the machine and do it manually.
I could blame it on sleep deprivation but my lack of software development skill was probably the biggest culprit. It was literally hacked together in the truest sense of the word.
v2.0# Fast forward to now. The AI hype cycle is at peak hype and billions of VC dollars have been invested in LLMs and other AI tech.
Then here I was, letting my AI tool languish on my server, not taking advantage of the progress of the last few years.
There were a few reasons I decided to dust this off:
I wanted to reorganize my server rack. I wanted to virtualise my truenas server (my old gaming machine). To do that, I needed to free up the PCI slot on my proxmox server that my old GPU was sitting it. That was the GPU I was using to do the transcripts. Due to a limitation of the motherboard on my proxmox server, I can only pass through that PCI slot, and I need it for my HBA card to give me enough SATA connections for my truenas hard drives. Ultimately, this would let me spin down that server and save a few bucks on electricity (or, lets be honest, use it for something else and not save electricity). ROCm finally doesn&rsquo;t suck butt, and I figured I could run the transcription process on my desktop now using my Radeon 6900xt. It would let me use a much larger (more accurate model) and would be faster than my macbook air or the 1070ti in the server. Which btw, I bought secondhand from my brother-in-law specifically to do the transcription work since it was so painful to use ROCm back then. I literally spent $175 to get an nvidia card to avoid the pain that was ROCm. The aforementioned fragileness of my old setup resulted in it needing more manual intervention than I had time to provide so it just languished, broken, for a few months. Part of the old system required me to manually go in and &ldquo;identify&rdquo; speakers then find/replace the &ldquo;SPEAKER_01&rdquo; with &ldquo;Leo Laporte&rdquo; etc. I knew that an LLM should be able to do this. Claude Code could, theoretically, fill in the programming skill gap that plagued my previous attempt. Technical Solution# Core Components# flowchart TD A[RSS Feeds] --> B[Audio Downloader] B --> C[Audio Storage] C --> D1[TranscriptionWhisper/WhisperX] C --> D2[Diarizationpyannote.audio] D1 --> E[Merge Segments] D2 --> E E --> F[LLM Speaker ID] F --> G[Export FormatsTXT/JSON/SRT] G --> H[File Management] Key Technologies# Transcription Engine: OpenAI Whisper Speaker Diarization: WhisperX + pyannote.audio LLM Integration: Optional GPT/Claude integration Infrastructure: Python 3.10+, AMD ROCm/NVIDIA CUDA support Unique Features# Three-Tier Transcript System# It generates three distinct versions of each transcript to serve different use cases:
1. Raw Transcription Output# Obviously a purely raw output is nice to have but its not the most &lsquo;usable&rsquo;.
Welcome to the Intelligent Machines podcast. Today we&#39;re diving deep into the world of artificial intelligence and machine learning. I&#39;m excited to have our guest here to discuss the latest developments in neural networks and their applications in real-world scenarios. The field has been advancing at an incredible pace, especially with the emergence of large language models and their impact on various industries. 2. Speaker-Diarized Version# The diarized version, aka the version with the speakers being differentiated, is even better.
SPEAKER_01: Welcome to the Intelligent Machines podcast. Today we&#39;re diving deep into the world of artificial intelligence and machine learning. SPEAKER_01: I&#39;m excited to have our guest here to discuss the latest developments in neural networks and their applications in real-world scenarios. SPEAKER_02: Thanks for having me on the show. SPEAKER_01: The field has been advancing at an incredible pace, especially with the emergence of large language models and their impact on various industries. SPEAKER_02: Absolutely, and what&#39;s particularly fascinating is how these models are being integrated into everyday applications. 3. LLM-Enhanced with Real Speaker Names# This is the real piece-de-resistance. Like I said before, I use to have to go into the diarized version (like you see above) and use context clues to figure out who SPEAKER_01 is. That was a very time consuming manual task and honestly I never got around to doing it for the thousands of episodes I transcribed and diarized.
But this new version uses an LLM to identify the speakers and make the changes. This part is trickier than it seems because the diarization part isn&rsquo;t the most accurate. It can get confused and think there are more speakers than their are because of Ads or things like overlapping conversation (which happens all of the time on podcasts and real life). Pyannote can struggle distinguishing speakers in those cases. But if you prompt an LLM well enough&hellip;
Leo Laporte: Welcome to the Intelligent Machines podcast. Today we&#39;re diving deep into the world of artificial intelligence and machine learning. Leo Laporte: I&#39;m excited to have our guest here to discuss the latest developments in neural networks and their applications in real-world scenarios. Dr. Sarah Chen: Thanks for having me on the show. Leo Laporte: The field has been advancing at an incredible pace, especially with the emergence of large language models and their impact on various industries. Dr. Sarah Chen: Absolutely, and what&#39;s particularly fascinating is how these models are being integrated into everyday applications. Automation Capabilities# RSS Feed Batch Processing# VoxDex can process multiple podcast feeds automatically by configuring RSS sources:
# config.yaml rss_feeds: - name: &#34;TWiT This Week in Tech&#34; url: &#34;https://feeds.twit.tv/twit.xml&#34; enabled: true - name: &#34;AI Podcast&#34; url: &#34;https://feeds.example.com/ai-podcast.xml&#34; enabled: true processing: max_episodes_per_run: 5 check_interval_hours: 6 # RSS processing example def process_rss_feeds(config): for feed in config[&#39;rss_feeds&#39;]: if feed[&#39;enabled&#39;]: episodes = fetch_new_episodes(feed[&#39;url&#39;]) for episode in episodes[:config[&#39;processing&#39;][&#39;max_episodes_per_run&#39;]]: download_and_process(episode) Configurable File Retention# Automatic cleanup based on age and storage limits:
# File retention configuration retention: audio_files: keep_days: 30 max_size_gb: 100 transcripts: keep_days: 365 backup_to_cloud: true temp_files: cleanup_immediately: true # Cleanup implementation def cleanup_old_files(): # Remove audio files older than 30 days for file in get_audio_files(): if file.age_days &gt; config.retention.audio_files.keep_days: file.delete() # Archive old transcripts to cloud storage for transcript in get_old_transcripts(): if config.retention.transcripts.backup_to_cloud: upload_to_cloud(transcript) Multiple Output Formats# Generate TXT, JSON, and SRT simultaneously:
# Export to multiple formats def export_transcript(segments, episode_metadata): base_filename = f&#34;{episode_metadata[&#39;show&#39;]}_{episode_metadata[&#39;date&#39;]}&#34; # Plain text format with open(f&#34;{base_filename}.txt&#34;, &#34;w&#34;) as f: for segment in segments: f.write(f&#34;{segment[&#39;speaker&#39;]}: {segment[&#39;text&#39;]}\\n\\n&#34;) # JSON format with metadata json_output = { &#34;metadata&#34;: episode_metadata, &#34;segments&#34;: [ { &#34;start_time&#34;: seg[&#39;start&#39;], &#34;end_time&#34;: seg[&#39;end&#39;], &#34;speaker&#34;: seg[&#39;speaker&#39;], &#34;text&#34;: seg[&#39;text&#39;] } for seg in segments ] } with open(f&#34;{base_filename}.json&#34;, &#34;w&#34;) as f: json.dump(json_output, f, indent=2) # SRT subtitle format with open(f&#34;{base_filename}.srt&#34;, &#34;w&#34;) as f: for i, segment in enumerate(segments, 1): f.write(f&#34;{i}\\n&#34;) f.write(f&#34;{format_time(segment[&#39;start&#39;])} --&gt; {format_time(segment[&#39;end&#39;])}\\n&#34;) f.write(f&#34;{segment[&#39;speaker&#39;]}: {segment[&#39;text&#39;]}\\n\\n&#34;) Sample Outputs# TXT Format (intelligent_machines_2024-10-05.txt):
Leo Laporte: Welcome to the Intelligent Machines podcast. Today we&#39;re diving deep into the world of artificial intelligence and machine learning. Leo Laporte: I&#39;m excited to have our guest here to discuss the latest developments in neural networks and their applications in real-world scenarios. Dr. Sarah Chen: Thanks for having me on the show. Leo Laporte: The field has been advancing at an incredible pace, especially with the emergence of large language models and their impact on various industries. Dr. Sarah Chen: Absolutely, and what&#39;s particularly fascinating is how these models are being integrated into everyday applications. JSON Format (intelligent_machines_2024-10-05.json):
{ &#34;metadata&#34;: { &#34;show&#34;: &#34;Intelligent Machines&#34;, &#34;date&#34;: &#34;2024-10-05&#34;, &#34;episode_title&#34;: &#34;Neural Networks in Practice&#34;, &#34;duration&#34;: &#34;3600&#34;, &#34;file_size&#34;: &#34;156MB&#34; }, &#34;segments&#34;: [ { &#34;start_time&#34;: &#34;00:00:00&#34;, &#34;end_time&#34;: &#34;00:00:08&#34;, &#34;speaker&#34;: &#34;Leo Laporte&#34;, &#34;text&#34;: &#34;Welcome to the Intelligent Machines podcast. Today we&#39;re diving deep into the world of artificial intelligence and machine learning.&#34; }, { &#34;start_time&#34;: &#34;00:00:08&#34;, &#34;end_time&#34;: &#34;00:00:16&#34;, &#34;speaker&#34;: &#34;Leo Laporte&#34;, &#34;text&#34;: &#34;I&#39;m excited to have our guest here to discuss the latest developments in neural networks and their applications in real-world scenarios.&#34; }, { &#34;start_time&#34;: &#34;00:00:16&#34;, &#34;end_time&#34;: &#34;00:00:18&#34;, &#34;speaker&#34;: &#34;Dr. Sarah Chen&#34;, &#34;text&#34;: &#34;Thanks for having me on the show.&#34; }, { &#34;start_time&#34;: &#34;00:00:18&#34;, &#34;end_time&#34;: &#34;00:00:28&#34;, &#34;speaker&#34;: &#34;Leo Laporte&#34;, &#34;text&#34;: &#34;The field has been advancing at an incredible pace, especially with the emergence of large language models and their impact on various industries.&#34; }, { &#34;start_time&#34;: &#34;00:00:28&#34;, &#34;end_time&#34;: &#34;00:00:35&#34;, &#34;speaker&#34;: &#34;Dr. Sarah Chen&#34;, &#34;text&#34;: &#34;Absolutely, and what&#39;s particularly fascinating is how these models are being integrated into everyday applications.&#34; } ] } SRT Format (intelligent_machines_2024-10-05.srt):
1 00:00:00,000 --&gt; 00:00:08,000 Leo Laporte: Welcome to the Intelligent Machines podcast. Today we&#39;re diving deep into the world of artificial intelligence and machine learning. 2 00:00:08,000 --&gt; 00:00:16,000 Leo Laporte: I&#39;m excited to have our guest here to discuss the latest developments in neural networks and their applications in real-world scenarios. 3 00:00:16,000 --&gt; 00:00:18,000 Dr. Sarah Chen: Thanks for having me on the show. 4 00:00:18,000 --&gt; 00:00:28,000 Leo Laporte: The field has been advancing at an incredible pace, especially with the emergence of large language models and their impact on various industries. 5 00:00:28,000 --&gt; 00:00:35,000 Dr. Sarah Chen: Absolutely, and what&#39;s particularly fascinating is how these models are being integrated into everyday applications. Technical Challenges &amp; Solutions# Speaker diarization accuracy: The pyannote project has made some great strides and is impressive software. There is a huge active community on huggingface.com supporting it. That said, it still struggles a bit at identifying unique speakers when
you don&rsquo;t tell it exactly how many voices it should expect (and that&rsquo;s not as straightforward as it seems because you have ads, they play clips of audio, etc.) people over-talk each other which happens all the time during normal conversation and especially on conversational podcasts where latency and your typical zoom lag come into play. Its good enough though, and when you combine it with an LLM you can pretty accurately identify who is saying what.
One thing I had to prompt the LLM with was letting it know that the same person could be SPEAKER_01 and SPEAKER_05 because of the imprecise nature of the diarization from pyannote. The LLM, initially, would only assign a name to one of those, which makes sense if you are an LLM and assume each &ldquo;SPEAKER_nn&rdquo; is unique. So, I had to let it know that different &ldquo;SPEAKER&quot;s could be the same person and to use further context clues.
GPU optimization: I have another post about getting ROCm working here. It was pretty seamless to get the whisperX code working locally with my GPU. I haven&rsquo;t done much in terms of optimizing. I changed to use the largest (English only) model which my GPU can handle. There is probably room for more tuning here. For now, I don&rsquo;t run it often enough and only do a handful of podcasts so if this isn&rsquo;t even going to move the needle on my electric bill.
LLM integration challenges: I wasn&rsquo;t having much luck with the smaller (cheaper) LLMs like gpt5nano. I could probably spend a little more time tuning my prompts or tweaking the sampling code to make it work better for me, but I decided to just throw the bigger model at it and call it a day. Its not too expensive at this point. I need to work out the math but its probably not much more than a small coffee at Starbucks (per month).
File management and processing pipeline: I added some configurable features to prune the downloaded podcasts. One of the issues I ran into with my old clunky setup was running out of disk space because I downloaded, and never deleted, ALL of the episodes on my limited VM filesystem.
# Retention configuration for managing storage retention: # Audio files (raw downloaded episodes) audio_files: keep_days: 7 # Delete after 1 week max_size_gb: 50 # Clean oldest when storage exceeds limit # Transcript files (.txt, .json, .srt) transcripts: keep_days: 365 # Keep transcripts for 1 year backup_before_delete: true backup_location: &#34;/path/to/backup&#34; # Temporary processing files temp_files: cleanup_immediately: true keep_on_error: true # Keep temp files if processing fails # Failed processing attempts failed_downloads: retry_after_days: 3 max_retries: 3 delete_after_days: 30 Next Steps# So whats next for this project? My plan is to run it for a month then check the results and performance. I am also planning on looking into a better way to index this for search or more meaningful use. Having raw text files is great because there are so many simple tools or utilities that can help dig through them (fzf, rg, telescope in neovim), but there is value in putting it into a database or Elasticsearch. There are some text analysis tools that could be fun to throw at it too (eg sentiment analysis, theme extraction, etc).
Other things I want to do at some point:
See if I can get a local LLM to perform as well as ChatGPT 5. It would be pretty cool to do all of this locally. (Though the cost to do this with the openai api is literally pennies). Create a utility script that can take my OLD diarized podcasts and run it through the LLM enhancement tool. I dont think I want to spend the time or energy re-processing thousands of old shows. The diarization was &ldquo;ok&rdquo; and I think the LLM enhancement piece would clean it up so its usable. Clean up my GitHub repository: https://github.com/wesgould/voxdex Getting started guide `}).add({id:1,href:"/recipes/roasted-tomato-burrata-dip/",title:"Roasted Tomato & Burrata Dip with Garlic Crostinis",description:`Stoney made this during his night to cook during the OBX trip in 2025. Ripe bursted cherry tomatoes, stir in burrata or stracciatella, and scoop it all up with crispy garlic crostinis.
Ingredients# # For the dip 2 cups cherry or grape tomatoes 2 Tbsp olive oil (or enough to coat the baking dish) \xBD tsp salt (or to taste) \xBC tsp freshly ground black pepper Pinch of red pepper flakes (optional, for heat) 1\u20132 cloves garlic, minced 1 Tbsp chopped fresh parsley 1 tsp dried oregano (or 1 Tbsp fresh, chopped) 1 burrata (or equivalent amount of stracciatella / burrata filling) Handful of fresh basil leaves, torn 2 Tbsp freshly grated Parmesan (more for serving, optional) # For the garlic crostinis 1 French baguette (or similar crusty bread) 2\u20133 Tbsp butter, softened 1 clove garlic, minced Pinch of salt 1 Tbsp chopped parsley 1 Tbsp grated Parmesan (optional) Instructions# 1. Preheat &amp; prep# Preheat your oven to 425 \xB0F (220 \xB0C).
`,content:`Stoney made this during his night to cook during the OBX trip in 2025. Ripe bursted cherry tomatoes, stir in burrata or stracciatella, and scoop it all up with crispy garlic crostinis.
Ingredients# # For the dip 2 cups cherry or grape tomatoes 2 Tbsp olive oil (or enough to coat the baking dish) \xBD tsp salt (or to taste) \xBC tsp freshly ground black pepper Pinch of red pepper flakes (optional, for heat) 1\u20132 cloves garlic, minced 1 Tbsp chopped fresh parsley 1 tsp dried oregano (or 1 Tbsp fresh, chopped) 1 burrata (or equivalent amount of stracciatella / burrata filling) Handful of fresh basil leaves, torn 2 Tbsp freshly grated Parmesan (more for serving, optional) # For the garlic crostinis 1 French baguette (or similar crusty bread) 2\u20133 Tbsp butter, softened 1 clove garlic, minced Pinch of salt 1 Tbsp chopped parsley 1 Tbsp grated Parmesan (optional) Instructions# 1. Preheat &amp; prep# Preheat your oven to 425 \xB0F (220 \xB0C).
2. Roast the tomatoes# Place the cherry tomatoes in a baking dish. Drizzle with olive oil, ensuring the bottom is coated. Sprinkle with salt, pepper, red pepper flakes (if using), minced garlic, parsley, and oregano. Toss gently to coat everything evenly. Bake for 20\u201330 minutes, until the tomato skins burst. If some hold their shape, gently press them with the back of a spoon to help them burst. 3. Make the crostinis# While tomatoes are roasting, slice the baguette into ~\xBC- to \xBD-inch slices. In a small bowl, mix softened butter, minced garlic, a pinch of salt, parsley, and Parmesan (if using). Spread this garlic butter mixture onto each bread slice. Arrange the slices on a baking sheet and toast for 5\u20137 minutes, or until golden brown and crunchy. You can do this simultaneously with the tomatoes if your oven has space. 4. Assemble and serve# When the tomatoes are done, remove the baking dish from the oven. Immediately top with torn basil, grated Parmesan, and the burrata (or burrata filling). Gently stir everything so the warm tomatoes mingle with the creamy cheese. Transfer to a serving bowl or keep in the baking dish. Serve hot, alongside the garlic crostinis. Tips &amp; notes# Use very ripe tomatoes\u2014they&rsquo;ll burst more easily and release more flavor. If your burrata is whole, you can cut it open and stir in the creamy center, or just place the whole ball on top and break it when serving. For extra garlic flavor, rub a toasted crostini with a raw cut garlic clove. For more heat, increase red pepper flakes or drizzle with spicy olive oil just before serving. `}).add({id:2,href:"/blog/shortcode-examples/",title:"Shortcode Examples",description:"Testing the new shortcodes implementation",content:`Shortcode Examples# This page demonstrates the newly implemented shortcodes.
Admonition Shortcodes# Note# Important Note This is a note admonition. It&rsquo;s useful for highlighting important information that readers should pay attention to. Info# Information This is an info admonition. Use this for general information that provides context or additional details. Tip# Pro Tip This is a tip admonition. Share helpful tips and tricks with your readers using this style. Success# Great Success This is a success admonition. Use this to highlight positive outcomes or successful implementations. Warning# Warning This is a warning admonition. Use this to alert readers about potential issues or things to be careful about. Error# Error This is an error admonition. Use this to highlight critical issues or problems that need attention. Example# Example Usage This is an example admonition. Use this to showcase code examples or demonstrate concepts.
echo &#34;Hello, World!&#34; Center Quote# The best way to predict the future is to invent it. Mermaid Diagrams# Flowchart# graph LR; A[Hard edge] -->|Link text| B(Round edge) B --> C{Decision} C -->|One| D[Result one] C -->|Two| E[Result two] Sequence Diagram# sequenceDiagram participant Alice participant Bob Alice->>John: Hello John, how are you? loop Healthcheck John->>John: Fight against hypochondria end Note right of John: Rational thoughts prevail! John-->>Alice: Great! John->>Bob: How about you? Bob-->>John: Jolly good! State Diagram# stateDiagram-v2 [*] --> Still Still --> [*] Still --> Moving Moving --> Still Moving --> Crash Crash --> [*] Class Diagram# classDiagram Animal <|-- Duck Animal <|-- Fish Animal <|-- Zebra Animal : +int age Animal : +String gender Animal: +isMammal() Animal: +mate() class Duck{ +String beakColor +swim() +quack() } class Fish{ -int sizeInFeet -canEat() } class Zebra{ +bool is_wild +run() } `}).add({id:3,href:"/blog/open-webui/",title:"Using open-webui as a local ChatGPT replacement",description:`Using open-webui# I set up a local LLM using ROCm on my desktop in my last post. So I have the LLM running locally, but its pretty clunky and as a paying user of ChatGPT, I want that cleaner UI/UX. Enter open-webui. This is exactly what I was looking for as a front-end for the ollama server I set up.
I decided to go with the Docker setup.
`,content:`Using open-webui# I set up a local LLM using ROCm on my desktop in my last post. So I have the LLM running locally, but its pretty clunky and as a paying user of ChatGPT, I want that cleaner UI/UX. Enter open-webui. This is exactly what I was looking for as a front-end for the ollama server I set up.
I decided to go with the Docker setup.
Since I fairly recently refreshed my install for Pop, I realized I didn&rsquo;t have Docker installed on my machine anymore. Setting up Docker on Pop is straightforward:
Install Docker# # Add Docker&#39;s official GPG key: sudo apt-get update sudo apt-get install ca-certificates curl sudo install -m 0755 -d /etc/apt/keyrings sudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc sudo chmod a+r /etc/apt/keyrings/docker.asc # Add the repository to Apt sources: echo \\ &#34;deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \\ $(. /etc/os-release &amp;&amp; echo &#34;$VERSION_CODENAME&#34;) stable&#34; | \\ sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null sudo apt-get update sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin You can check that Docker is installed by running docker --version. I prefer Docker Compose when I run containers.
Docker Compose Configuration# This is what my compose file looks like:
version: &#39;3.8&#39; # Specify the version of Docker Compose services: open-webui: image: ghcr.io/open-webui/open-webui:main container_name: open-webui environment: - OLLAMA_BASE_URL=http://127.0.0.1:11434 volumes: - /location/on/host/tosavedata:/app/backend/data network_mode: host restart: always At this point, when you go to localhost:8080 you will see the web front end.
The Interface# The service makes you create an account. Its all local. Nothing is sent anywhere. There is a flag you can set when you spin up your docker container, but I decided to leave it this way since I may eventually set this up so my wife can use it too.
The interface is very familiar if you&rsquo;ve used ChatGPT before. Up at the top you can select your model to use. Its worth setting a default. It doesn&rsquo;t make you set a default, but it gets pretty annoying to open a new chat and have to manually pick a model each time. In my opinion, they should have it select a default on first use, but its not a huge deal at the end of the day.
And that&rsquo;s really it. Now you have a ChatGPT like UX that lets you quickly switch between models, remembers your chats if you need to go back and reference them, and its all running locally.
I plan on running LM Studio as well. Its a little more conducive to &lsquo;playing&rsquo; with the models and prompts and it gives easier access to a lot of the uncensored models that don&rsquo;t have the nanny protections built in. I also plan on setting up a RAG (Retrieval Augmented Generation)with some of my own data to see how useful I can make all of this.
`}).add({id:4,href:"/blog/rocm-on-pop/",title:"Running an LLM locally on Pop!_OS with ROCm support",description:`Running ROCm on Pop!# This has gotten sooo much easier than when I tried to set this up 2 years ago. ROCm has come a long way, but the support and tooling has advanced as well. Now you don&rsquo;t have to jump through tons of hoops to get AI libraries and software to work with ROCm. The last time I tried this, I needed to add ubuntu repos, edit my /etc/os-release file to pretend I was using ubuntu&quot;, and do a rain-dance to get my Raedeon 6900xt in a usable state.
`,content:`Running ROCm on Pop!# This has gotten sooo much easier than when I tried to set this up 2 years ago. ROCm has come a long way, but the support and tooling has advanced as well. Now you don&rsquo;t have to jump through tons of hoops to get AI libraries and software to work with ROCm. The last time I tried this, I needed to add ubuntu repos, edit my /etc/os-release file to pretend I was using ubuntu&quot;, and do a rain-dance to get my Raedeon 6900xt in a usable state.
Then once that was done, you realize that none of the software and libraries incorporated ROCm support and get really sad that you sold your Nvidia GFX card because &ldquo;radeon works better on linux&rdquo;. Basically, 2 years ago, getting ROCm setup was a pain in the ass and your only reward was buggy or nonfunctioning software since the AI/ML world is built on CUDA. Now it&rsquo;s much better with ROCm supporting CUDA and/or most of the libraries incorporating ROCm. Things like pytorch now support AMD ROCm natively.
Getting ROCm installed on Pop!_OS has been simplified to copying and pasting some commands:# wget https://repo.radeon.com/rocm/rocm.gpg.key -O - | gpg --dearmor | sudo tee /etc/apt/keyrings/rocm.gpg &gt; /dev/null echo &#34;deb [arch=amd64 signed-by=/etc/apt/keyrings/rocm.gpg] https://repo.radeon.com/rocm/apt/6.1 jammy main&#34; | sudo tee --append /etc/apt/sources.list.d/rocm.list echo -e &#39;Package: *\\nPin: release o=repo.radeon.com\\nPin-Priority: 600&#39; | sudo tee /etc/apt/preferences.d/rocm-pin-600 Then update and install:
sudo apt update &amp;&amp; sudo apt install rocm Add user to the &lsquo;render&rsquo; group so your user profile has permission to use it.
sudo usermod -a -G render $USER You can confirm it all works with:
rocminfo Installing the actual language models:# Since we have ROCm installed and working properly, let&rsquo;s install a local LLM. I decided to use Ollama, which is Meta&rsquo;s open source model. Ollama.ai actually has a script that helps get it all up-and-running quickly. They even have a Linux version.
I hate that this has become a common practice in the *nix community, because it&rsquo;s such a huge security risk, but they offer a script you can run if you copy and paste this command:
curl -fsSL https://ollama.com/install.sh | sh Only do this if you have reviewed the script and/or can REALLY trust the source. You are downloading a script from the internet and piping it into your shell to run.
There are manual steps here, though, that you can follow if you don&rsquo;t want to use the script: https://github.com/ollama/ollama/blob/main/docs/linux.md
I ran the script (after I reviewed it!). If you&rsquo;ve done everything properly, you should be able to type:
ollama run llama3 The first time you run this it will download llama3, which can take a bit, but it will drop you into a prompt. Where you can talk to it like you would ChatGPT or any other LLM.
And thats that! A working LLM on Pop!_OS using ROCm on your Radeon GFX card! The next step is adding a more user friendly, ChatGPT-like interface for it. This way you can discover and switch models easily, save your history, and easily set system prompts and your own guardrails&ndash;all locally, and privately. That will be on the next post when I have some time to tinker with it.
`}).add({id:5,href:"/blog/pop-refresh/",title:"Pop Refresh",description:`Pulled the trigger on a Pop_OS refresh# I&rsquo;ve been having an issue with Apex Legends. It&rsquo;s gotten progressively worse of the last few months to the point where it just doesn&rsquo;t work anymore. Thats the story for another blog post but I decided to go &ldquo;nuclear&rdquo; and do a pop-refresh https://support.system76.com/articles/pop-recovery
I figured it wouldn&rsquo;t be a bad idea to clear out all of those config tweaks, orphaned programs, and all of that other stuff that, over years of use, builds up on your system.
`,content:`Pulled the trigger on a Pop_OS refresh# I&rsquo;ve been having an issue with Apex Legends. It&rsquo;s gotten progressively worse of the last few months to the point where it just doesn&rsquo;t work anymore. Thats the story for another blog post but I decided to go &ldquo;nuclear&rdquo; and do a pop-refresh https://support.system76.com/articles/pop-recovery
I figured it wouldn&rsquo;t be a bad idea to clear out all of those config tweaks, orphaned programs, and all of that other stuff that, over years of use, builds up on your system.
The Pop Refresh Instructions Didn&rsquo;t Work For Me Because# The pop refresh instructions didn&rsquo;t work for me because:
The recovery partition was b0rked
I had to create a live usb and do the refresh from there. Nothing I did could fix the recovery partition and it would just boot my into busybox initramfs where I couldn&rsquo;t do anything. It was a little disconcerting at first and a little panic inducing because you dont know wtf happened or if your data just got all screwed. I kept reminding myself I have several backups so if I had to start from scratch I could.
But, with my trusty live USB, I was able to execute the &ldquo;refresh&rdquo; install.
What none of the instructions tell you
What none of the instructions tell you is that when it logs back in you have to create a new account as if you are starting from scratch. Once you do that, you have to edit a users file because they make your old account a &ldquo;system account&rdquo;, so it wont show up in the gdm login page. Once you do that it shows up along with the new user you just created. Thankfully, everything is where you left it.
All non-system apps are gone
All non-system apps are gone. They warn you about this but what they dont tell you is that flatpaks dont go anywhere. And that makes sense because they are saved in your home folder and nothing in there was touched, but its worth nothing that any flatpaks will still be there and still function.
Signal though kinda wigged out on me
Signal though kinda wigged out on me. I ended up making a backup of the flatpak structure then reinstalling it. Obviously it doesn&rsquo;t retain the history, but I found I could copy and paste the /.var/app/org.signal.Signal/config/Signal from the backup to the new installs config directory and it worked&ndash;it pulled in all of my old messages etc. Note: I had already logged in and got Signal up and running before I moved the data from the backup. I named it the same thing as before (but I am not sure if that matters). This is probably worth a post by itself tbh.
Neovim was a bit of a pain to reinstall
Neovim was a bit of a pain to reinstall. I dont recall how I had installed it before but I reinstalled from source this time which changed the versions and broke my config. I was using a fairly modified kickstart.nvim config so I had to work through a few issues before I got it up and running again.
Syncthing was super easy to reinstall
Syncthing was super easy to reinstall actually once I remembered the steps. After I installed the .deb I went to the admin url and it just had all of my settings&ndash;which was nice.
Tailscale seems to have installed pretty seamlessly
Tailscale seems to have isntalled pretty seamlessly. It did create another version of my machine. So I had to manually remove the old host info, but I made sure I grabbed the acl settings from it beforehand. Hopefully I didn&rsquo;t miss anything but time will tell and its all fixable.
Programs I Had to Reinstall:# Neovim compile pre-requisites htop neofetch openssh-server - it seems like the known-hosts file will need to be updated for everything that connects to it. makes sense, but a little annoying. ripgrep zsh fzf syncthing tailscale dejadup - I had the option to install the flatpak, but because I didn&rsquo;t still have it on my system, I knew I installed it via .deb previously and I wanted to see if it would just use my config settings (that should still exist)&ndash; which it did. I guess in the future I could move to the flatpak version and it would remain installed during a refresh. steam &ndash; I used the deb. I&rsquo;ve bounced around from flatpak to deb multiple times. I gave the deb a shot because I have been having plenty of issues with apex on steam and getting banned from The Finals because ??? I&rsquo;m sure there will be plenty more programs I&rsquo;ll realize I need as I use my computer, but it&rsquo;s nice to start with a clean slate (edit: like all of the pre-requisites to run a hugo site like hugo, go, git, etc).
`}).add({id:6,href:"/recipes/stuffed-peppers/",title:"Stuffed Peppers",description:`These stuffed peppers are filled with a hearty mix of rice, ground meat, and tomato, then baked until the peppers are tender and everything comes together in one simple dish.
`,content:`These stuffed peppers are filled with a hearty mix of rice, ground meat, and tomato, then baked until the peppers are tender and everything comes together in one simple dish.
Ingredients# 3 bell peppers 2 Tbsp cooking oil, divided 1 lb Italian sausage 1 yellow onion, diced 3 garlic cloves, minced 1 tsp Italian seasoning 1/2 tsp garlic powder 1 1/4 tsp salt, divided 1/4 tsp freshly cracked black pepper 1 cup marinara sauce 1/2 cup uncooked long grain white rice 3/4 cup chicken broth 1 cup shredded mozzarella Instructions# Preheat the oven to 350\xB0F. Wash and dry each bell pepper, then cut the bell peppers in half horizontally. Make sure to cut them as evenly as possible. Using a sharp paring knife carefully cut and remove the stem from the top half of each bell pepper (see picture below). It&rsquo;s okay if there is a small hole left where the stem was removed. Prepare the bell peppers. Place each bell pepper half in a 9\xD713-inch casserole dish. Brush the bell peppers with 1 Tbsp oil and season with \xBC tsp of salt and \xBC tsp cracked black pepper. Bake the bell peppers in a preheated oven for 20 minutes to soften. After 20 minutes remove the bell peppers from the oven and set aside. Make the filling. While the bell peppers are baking, heat a large skillet over medium heat and add 1 Tbsp of oil. Brown the Italian sausage. Add the vegetables. Once the sausage has browned, add the diced onion and minced garlic to the skillet. Continue to saut\xE9 over medium heat until the onion is translucent and the garlic is fragrant. Combine remaining ingredients. Next add the uncooked rice, marinara sauce, Italian seasoning, garlic powder, 1 tsp of salt, and chicken broth to the skillet. Stir to combine. Cook the filling. Place a lid on the skillet, turn the heat up to medium-high, and allow the mixture to come to a full boil. Once boiling, immediately reduce the heat to medium-low and allow the mixture to simmer, without lifting the lid or stirring, for 20 minutes. After 20 minutes, turn the heat off and let it rest, without lifting the lid, for an additional 5 minutes. Fill the bell peppers. Next remove the lid, fluff the rice, and stir the mixture again to redistribute the ingredients. Begin to fill each bell pepper with the meat filling. Stuff as much filling as you can into each, filling them all the way to the top. Add cheese and bake. Top each bell pepper evenly with shredded mozzarella cheese. Loosely place some tented aluminum foil over the top of the casserole dish and bake for 15 minutes. After 15 minutes the bell peppers should be tender but not mushy. Broil the bell peppers. Now remove the foil and turn the heat on to broil. Broil the stuffed bell peppers for 2-3 minutes or just until the cheese gets a little brown on top. Be sure to watch the bell peppers closely at this step to prevent the cheese from over browning. `}).add({id:7,href:"/blog/migration-pt2/",title:"Migration Pt2",description:`Updating DNS Records# This was a fairly straightforward process:
Log into GitHub &gt; Go to your public repo &gt; Settings &gt; Pages: and enter your domain in the Custom Domain section Add your domain name www.wesgould.com Log into your DNS provider. Delete CNAME / A-RECORDs for www.wesgould.com and wesgould.com to avoid conflicts with the new records. ADD CNAME RECORD for www.wesgould.com. Add A-RECORD for wesgould.com. To create A records, point your apex domain to the IP addresses for GitHub Pages: - 185.199.108.153 - 185.199.109.153 - 185.199.110.153 - 185.199.111.153 `,content:`Updating DNS Records# This was a fairly straightforward process:
Log into GitHub &gt; Go to your public repo &gt; Settings &gt; Pages: and enter your domain in the Custom Domain section Add your domain name www.wesgould.com Log into your DNS provider. Delete CNAME / A-RECORDs for www.wesgould.com and wesgould.com to avoid conflicts with the new records. ADD CNAME RECORD for www.wesgould.com. Add A-RECORD for wesgould.com. To create A records, point your apex domain to the IP addresses for GitHub Pages: - 185.199.108.153 - 185.199.109.153 - 185.199.110.153 - 185.199.111.153 To create AAAA records, point your apex domain to the IP addresses for GitHub Pages: - 2606:50c0:8000::153 - 2606:50c0:8001::153 - 2606:50c0:8002::153 - 2606:50c0:8003::153 Be impatient because &ldquo;it didn&rsquo;t work!&rdquo; (immediately). But it&rsquo;s DNS propagation and it&rsquo;s slow. I understand why it takes a while, and theoretically it could take days to fully propagate, but it also seems crazy that it&rsquo;s not instantaneous in 2023. That said, it probably took me longer to complain here than it did to propagate. Try accessing the site. GitHub may report an error at first under where you entered your custom domain, but check again, and it should work. wesgould.com is improperly configured Domain does not resolve to the GitHub Pages server. For more information, see documentation (NotServedByPagesError). Make sure you check the HTTPS box. Do one last terminal dig to see the changes like a nerd. Bug your wife and friends to see if they can get to your site. Pretend they are as excited as you are even though &ldquo;Okay now what&rdquo; is about all you&rsquo;ll get from them. That&rsquo;s an exact quote from my wife. `}).add({id:8,href:"/recipes/homebakes/",title:"Homebakes",description:`Home Bakes# Family recipe from Oma passed down.
Ingredients# 2 cups Sugar 4 Tbls. Cocoa 1 stick Butter \xBD cup Milk \xBD cup Peanut Butter 2 \xBD cups 1 Minute Oatmeal Directions:# 1. Lay down two strips of waxed paper. (Foil can also be used in a pinch.)
2. In a large saucepan, combine sugar and cocoa. Stir until well blended.
`,content:`Home Bakes# Family recipe from Oma passed down.
Ingredients# 2 cups Sugar 4 Tbls. Cocoa 1 stick Butter \xBD cup Milk \xBD cup Peanut Butter 2 \xBD cups 1 Minute Oatmeal Directions:# 1. Lay down two strips of waxed paper. (Foil can also be used in a pinch.)
2. In a large saucepan, combine sugar and cocoa. Stir until well blended.
3. Add milk and stir. Then add a stick of butter.
4. Place over medium heat and cook until the mixture comes to a complete rolling boil, stirring often. Boil for 2 minutes (use a timer or watch the clock).
5. Remove from the heat source.
6. Add peanut butter and stir until it is all melted.
7. Add oats and stir.
8. Using a tablespoon, immediately spoon the mixture onto wax paper, working quickly.
9. Allow the cookies to cool; they will set as they cool.
`}).add({id:9,href:"/blog/migrating-to-hugo/",title:"Migrating to Hugo",description:`It&rsquo;s been an&hellip;adventure.
Why Switch from Publii to Hugo?# I am switching from Publii to Hugo because I wanted to take advantage of the text notes I was already taking with Vimwiki for my various projects. The Publii interface was nice, but I&rsquo;m already writing notes in a format that doesn&rsquo;t require me to use their WYSIWYG editor. I know that seems lazy, but it&rsquo;s an extra step that caused just enough friction that I wouldn&rsquo;t update the blog.
`,content:`It&rsquo;s been an&hellip;adventure.
Why Switch from Publii to Hugo?# I am switching from Publii to Hugo because I wanted to take advantage of the text notes I was already taking with Vimwiki for my various projects. The Publii interface was nice, but I&rsquo;m already writing notes in a format that doesn&rsquo;t require me to use their WYSIWYG editor. I know that seems lazy, but it&rsquo;s an extra step that caused just enough friction that I wouldn&rsquo;t update the blog.
I picked Hugo because it seemed simple, used text files vs. databases, etc., and honestly, I found a sick Gruvbox based theme that I liked. Gruvbox is the one true color palette.
Working through the kinks# Setting it up was actually really easy. I got it all working locally in about an hour, but when I went to deploy it to the S3 bucket, it just didn&rsquo;t work. I discovered that AWS S3 buckets don&rsquo;t like &ldquo;pretty URLs&rdquo;. In other words, S3 buckets need the URL with the &ldquo;.html&rdquo; at the end, or it throws an error.
The fix is using &ldquo;ugly URLs&rdquo;. This argument, added to the Hugo config.toml, should add the .html to the end of all of the URLs. It does that, but it also provides the ENTIRE URL. So, if you clicked the link to the &lsquo;blog&rsquo;, instead of going to wesgould.com/blog.html, it would go to wesgould.com/www.wesgould.com/blog.html. So either Hugo has a bug in it, or the way my particular theme is set up, that ALSO didn&rsquo;t work. After fighting it for entirely too long, I decided that I didn&rsquo;t want to recode the entire theme. I&rsquo;d just move from AWS to GitHub pages&ndash;which does support prettyURLs.
It seemed simple and would save me $0.50 a month (since getting TLS certs requires CloudFront, which costs the exorbitant $0.50/mo). It also aligned with the Git workflow I was already using.
I read a couple of tutorials but ultimately ended up following this amazing YouTube video. It walks you through creating a &ldquo;working code&rdquo; repository and a public repository where you can publish your static site content.
Hooray! This will be easy peasy!
But, I ran into another problem and beat my head against the wall. The site deployed and seemed to work fine, except for the Blog URL. It was directing me to a weird readme. The source of the page referenced Jekyll and SEO optimizations. I grepped for those strings in my code and couldn&rsquo;t find them. Long story short, the culprit was a &ldquo;gh-pages&rdquo; branch of code in my website repo. Having a gh-pages branch (designed for Jekyll) in your repo tells GitHub to kick off an automated workflow to deploy a static site. Cool.
I&rsquo;d started to follow this tutorial before I found that YouTube video. So, I had created a gh-pages branch and totally forgot about it. Apparently, that magical branch automatically injects stuff into your code/site as it&rsquo;s updated\u2014and in my case, broke the &ldquo;blog&rdquo; links. Deleting that branch fixed all of the problems.
It&rsquo;s working now, and I am looking forward to tuning it. The only thing I don&rsquo;t like is that it, probably correctly, follows the system setting for light vs. dark mode, even though I&rsquo;ve set the default to dark. Light mode Gruvbox is gross. I might try to fix it and submit a PR to the theme editor. I&rsquo;ve seen other themes that let users choose the system, light, or dark.
The last thing I need to do is clean up DNS so my actual domain displays this instead of my broken S3 version.
`}).add({id:10,href:"/blog/creating-nfs-shares/",title:"Creating NFS Shares",description:`Creating an NFS share# If you are trying to share between *nix systems, the Network File Servers (NFS) are really easy to set up. Note: If you are planning on sharing with Windows machines, SMB/Samba is a better option.
It should also be noted that NFS doesn&rsquo;t restrict, by default, to individual users. It limits to IPs and IP ranges&ndash;which makes it ideal for secure networks, but less-than-ideal for non-secure / public networks.
`,content:`Creating an NFS share# If you are trying to share between *nix systems, the Network File Servers (NFS) are really easy to set up. Note: If you are planning on sharing with Windows machines, SMB/Samba is a better option.
It should also be noted that NFS doesn&rsquo;t restrict, by default, to individual users. It limits to IPs and IP ranges&ndash;which makes it ideal for secure networks, but less-than-ideal for non-secure / public networks.
It&rsquo;s pretty straight forward (on debian/apt bases distributions):
Installing the NFS server# sudo apt-get update &amp;&amp; sudo apt-get install nfs-kernel-server Make the directory you would like to share. This directory can go anywhere but I think standard location is in the /mnt/ directory.
sudo mkdir -p /mnt/your-directory-to-share This (below) changes the permissions to let all of the client machines access to the directory. You can change the permissions as well but for now I left it
sudo chown -R nobody:nogroup /mnt/your-directory-to-share\` Give the clients read,write, and execute permissions as well:
sudo chmod -R 777 /mnt/your-directory-to-share\` At this point your nfs-server is installed (not active) and your shared folder is setup. Now you need to grant access to the NFS share. To do that you have to edit the &rsquo;exports&rsquo; file&ndash;which is found at /etc/exports.
sudo vim /etc/exports /mnt/your-directory-to-share &lt;ipaddress of the client&gt;(rw,sync,no_subtree_check) If you want to add specific IPs you have to create a new line for each of them. However, if you wanted to do an entire subnet/ip range you can write something like this: 192.168.1.0/24 (which would share 192.168.1.1-255)
rw = read/write ; sync = means changes have to be written to disk before they are applied ; no_subtree_check = means what it says. Apparently, subtree checking causes more problems than its worth, so most people recommend using this flag to turn it off.
According the NFS man pages and tutorials I found you are supposed to run the following in this order:
sudo exportfs -a sudo systemctl restart nfs-kernel-server This threw an error for me. I ran them a second time and it seems to work. I think you have to have the nfs-kernel-server running before you can do sudo exportfs -a. When you run the restart command it actually starts it for the first time, then you can follow the steps properly.
If it was up to me, I think it should be:
sudo systemctl start nfs-kernel-server sudo exportfs -a sudo systemctl restart nfs-kernel-server Connect your client# So now you have the server serving up the shared drive. Now you need to connect to it.
On your laptop/pc/whatever is connecting to the NFS server you need to install the NFS client and mount the drive.
sudo apt-get install nfs-common Make a directory/mount point for this. Again, this can be anywhere. If you wanted it to be in your home folder it can be, but we&rsquo;ll stick with /mnt/nfsshare
sudo mkdir -p /mnt/nfsshare Then you mount it:
sudo mount &lt;ip-of-NFS-server&gt;:/mnt/your-directory-to-share /mnt/nfsshare And thats it. When you navigate to /mnt/nfsshare on your client machine you will see all of the contents of the nfs drive. Which at this point would be empty.
If you want to mount it permanently, just edit your /etc/fstab file by adding a line at the end:
ip-of-NFS-server&gt;:/mnt/your-directory-to-share /mnt/nfsshare nfs defaults 0 0 This article was updated on October 17, 2021
`}).add({id:11,href:"/recipes/enchilada-casserole/",title:"Enchilada Casserole",description:`One of Mom\u2019s favorite recipes from Kris H. &amp; Robin K.
Ingredients# 2 cups grated cheddar cheese 1/2 cup finely chopped green onion 2 cans cream of chicken soup 1 cup sour cream 1 can chopped green chilies 1/2 tsp salt 2 cans chicken (all white meat) OR 4 chicken breasts, cooked and chopped 12 corn tortillas, hand shredded Important Ingredient Note Use Chopped Green Chilies from the Mexican Food section - these come in very small cans. DO NOT GET FIRE-ROASTED versions! The cans look almost identical but the taste is completely different. Chicken Cooking Tip Cook raw chicken breasts by placing in a sauce pan, just cover with water, and bring to boil. Turn heat to low and simmer for 30 minutes to 1 hour until no pink remains in center. Remove chicken, cool, and chop into bite-sized pieces. This keeps the meat tender and moist. Instructions# Preheat oven to 350\xB0F.
`,content:`One of Mom\u2019s favorite recipes from Kris H. &amp; Robin K.
Ingredients# 2 cups grated cheddar cheese 1/2 cup finely chopped green onion 2 cans cream of chicken soup 1 cup sour cream 1 can chopped green chilies 1/2 tsp salt 2 cans chicken (all white meat) OR 4 chicken breasts, cooked and chopped 12 corn tortillas, hand shredded Important Ingredient Note Use Chopped Green Chilies from the Mexican Food section - these come in very small cans. DO NOT GET FIRE-ROASTED versions! The cans look almost identical but the taste is completely different. Chicken Cooking Tip Cook raw chicken breasts by placing in a sauce pan, just cover with water, and bring to boil. Turn heat to low and simmer for 30 minutes to 1 hour until no pink remains in center. Remove chicken, cool, and chop into bite-sized pieces. This keeps the meat tender and moist. Instructions# Preheat oven to 350\xB0F.
Prepare the filling. Mix in a large sauce pan the chopped cooked chicken breasts, cream of chicken soup, sour cream, and green chilies. Stir until blended over low heat.
Combine ingredients. Remove from heat and add cheddar cheese, green onion, and tortillas. Add salt and pepper to taste.
Assemble casserole. Place all ingredients in a 9&quot; x 13&quot; casserole dish and top lightly with grated cheese.
Bake the casserole. Bake for 20 to 30 minutes at 350\xB0F until bubbling.
Serving Suggestion This casserole is delicious on its own or served with Tostitos for extra crunch! `}).add({id:12,href:"/recipes/cincinatti-chili/",title:"Cincinatti Chili",description:`Ingredients# # For the chili 2 pounds ground beef (80:20 is good; 90:10 works as well and will obviously be less fatty) 1 6-ounce can tomato paste 4 cups water 1 (8-ounce) can tomato sauce 1 large onion, minced 6 cloves garlic, minced (pre-minced in the jar is fine) 3 tablespoons chili powder 1 teaspoon cumin 1 teaspoon cinnamon 3/4 teaspoon ground allspice 1/4 teaspoon ground cloves 1/2 teaspoon cayenne 2 teaspoons kosher salt 2 tablespoons Worcestershire sauce TIP: Measure all of your ingredients first. I usually set all of the spices on my left with a small bowl in front of me. As I measure and put each ingredient into the bowl, I move the spice bottle to the right side so I know I&rsquo;ve added it. It&rsquo;s really easy to forget which spices you&rsquo;ve measured out!
`,content:`Ingredients# # For the chili 2 pounds ground beef (80:20 is good; 90:10 works as well and will obviously be less fatty) 1 6-ounce can tomato paste 4 cups water 1 (8-ounce) can tomato sauce 1 large onion, minced 6 cloves garlic, minced (pre-minced in the jar is fine) 3 tablespoons chili powder 1 teaspoon cumin 1 teaspoon cinnamon 3/4 teaspoon ground allspice 1/4 teaspoon ground cloves 1/2 teaspoon cayenne 2 teaspoons kosher salt 2 tablespoons Worcestershire sauce TIP: Measure all of your ingredients first. I usually set all of the spices on my left with a small bowl in front of me. As I measure and put each ingredient into the bowl, I move the spice bottle to the right side so I know I&rsquo;ve added it. It&rsquo;s really easy to forget which spices you&rsquo;ve measured out!
Directions# Cook the tomato paste: Heat a large, heavy-bottomed pot or Dutch oven over medium-high heat. Add the tomato paste to the dry pot and cook, constantly scraping the bottom with a wooden spoon or silicone spatula, until the tomato smells rich and toasty and you start to see browned (not burned) patches in the bottom of the pot. This should take 1 to 3 minutes.
Combine the ingredients in a pot: Remove the pot from heat and add the ground beef and water. Mix them together into a sludge. It will not look pretty, but press on. There&rsquo;s a method to this madness. Return to medium-high heat and bring to a simmer, stirring all the while, so the sludge breaks up into a mealy paste. Add all the remaining ingredients.
Simmer gently, uncovered, for 2 to 3 hours: Stir the chili often. You want the volume to reduce a bit. If it starts to lose too much water and is getting too thick, reduce the heat and cover with a lid&ndash;leaving just a bit for some steam to escape. It will be ready in an hour, but the longer you let it simmer the richer the flavor will be.
`}).add({id:13,href:"/blog/embed-video-files/",title:"Embed Video Files",description:`Use the video shortcode to embed your video files from Hugo Page Resources.
`,content:`Use the video shortcode to embed your video files from Hugo Page Resources.
With a page bundle looking like the following:
embed-videos/ |-- index.md |-- my-video.jpg |-- my-video.mp4 |-- my-video.webm You can embed my-video like this:
{{&lt; video src=&#34;my-video&#34; autoplay=&#34;true&#34; controls=&#34;false&#34; loop=&#34;true&#34; &gt;}} The shortcode looks for media files matching the filename my-video*. For each video MIME type file, a &lt;source&gt; element is added. The first image MIME type file is used as poster (thumbnail). It will render the following HTML:
&lt;video autoplay loop poster=&#34;/blog/embed-videos/my-video.jpg&#34; width=&#34;100%&#34; playsinline &gt; &lt;source src=&#34;/blog/embed-videos/my-video.mp4&#34; type=&#34;video/mp4&#34; /&gt; &lt;source src=&#34;/blog/embed-videos/my-video.webm&#34; type=&#34;video/webm&#34; /&gt; &lt;/video&gt; You can set a Markdown caption, wrapping the &lt;video&gt; inside a &lt;figure&gt;.
Additionally, the shortcode allows you to set the following attributes:
Attribute Default autoplay false controls true height loop false muted true preload width 100% playsinline true Learn more about the &lt;video&gt; attributes here.
`}).add({id:14,href:"/blog/image-optimization/",title:"Image Optimization",description:`The theme optimizes images by default with a custom Hugo&rsquo;s markdown render hook:
The theme creates resized versions for each image, ranging from 100 to 700 pixels wide. It generates WebP versions for each size if the original image format isn&rsquo;t WebP. The theme keeps the original file format as a fallback for browsers that don&rsquo;t support the WebP format. Images in SVG format are embedded as-is. Blog Post Cover Images# Use the front matter of your posts to add cover images:
`,content:`The theme optimizes images by default with a custom Hugo&rsquo;s markdown render hook:
The theme creates resized versions for each image, ranging from 100 to 700 pixels wide. It generates WebP versions for each size if the original image format isn&rsquo;t WebP. The theme keeps the original file format as a fallback for browsers that don&rsquo;t support the WebP format. Images in SVG format are embedded as-is. Blog Post Cover Images# Use the front matter of your posts to add cover images:
--- cover: src: alexandre-van-thuan-mr9FouttLGY-unsplash.jpg alt: The interior of Stadsbiblioteket in Stockholm - Gunnar Asplunds library from 1928. The architecture is a transition between neoclassicism and functionalism. caption: By [Alexandre Van Thuan](https://unsplash.com/photos/mr9FouttLGY) --- Captions# Add captions to your inline images like this:
--- ![Alt text](image-url.jpg &#34;Caption with **markdown support**&#34;) --- The main library in Vancouver is architecturally significant. The angles and levels contour together to produce a trippy scene. It&rsquo;s pretty from the outside but stunning from the inside. By Aaron Thomas JPEG and WebP Quality# The default quality is 75%. See the official Image Processing Config Hugo docs. Change it by adding the following to the config.toml file:
[imaging] quality = 75 Resizing# By default, the theme creates resized versions of images ranging from 300 to 700 pixels wide in increments of 100 pixels. Override the resize behavior by adding the following to the config.toml file:
[params] [params.imageResize] min = 300 max = 700 increment = 100 Lazy Loading# Images are lazily loaded by default using the loading=&quot;lazy&quot; attribute on HTML img tags.
`}).add({id:15,href:"/blog/turkey-trot-2019/",title:"Turkey Trot 2019",description:`Turkey Trot 2k19# Date: November 28, 2019
The family decided to go to a Turkey Trot before we ate Thanksgiving dinner. I bet my niece $1 that I would run it in under 40 minutes. I got &lt; 33 minutes (unofficially\u2014the times aren&rsquo;t posted yet).
About 900 people signed up for this particular 5k!
The 2nd prize was a huge pie (and a $50 Dick&rsquo;s gift card, but you can&rsquo;t eat that so it&rsquo;s less important).
`,content:`Turkey Trot 2k19# Date: November 28, 2019
The family decided to go to a Turkey Trot before we ate Thanksgiving dinner. I bet my niece $1 that I would run it in under 40 minutes. I got &lt; 33 minutes (unofficially\u2014the times aren&rsquo;t posted yet).
About 900 people signed up for this particular 5k!
The 2nd prize was a huge pie (and a $50 Dick&rsquo;s gift card, but you can&rsquo;t eat that so it&rsquo;s less important).
Disclaimer: This post was narrated by my chief editor\u2014 my niece.
This article was updated on August 12, 2020
`}).add({id:16,href:"/blog/placeholder-text/",title:"Placeholder Text",description:"Lorem Ipsum Dolor Si Amet",content:`Lorem est tota propiore conpellat pectoribus de pectora summo.
Redit teque digerit hominumque toris verebor lumina non cervice subde tollit usus habet Arctonque, furores quas nec ferunt. Quoque montibus nunc caluere tempus inhospita parcite confusaque translucet patri vestro qui optatis lumine cognoscere flos nubis! Fronde ipsamque patulos Dryopen deorum.
Exierant elisi ambit vivere dedere Duce pollice Eris modo Spargitque ferrea quos palude Rursus nulli murmur; hastile inridet ut ab gravi sententia! Nomine potitus silentia flumen, sustinet placuit petis in dilapsa erat sunt. Atria tractus malis.
Comas hunc haec pietate fetum procerum dixit Post torum vates letum Tiresia Flumen querellas Arcanaque montibus omnes Quidem et Vagus elidunt# The Van de Graaf Canon
Mane refeci capiebant unda mulcebat# Victa caducifer, malo vulnere contra dicere aurato, ludit regale, voca! Retorsit colit est profanae esse virescere furit nec; iaculi matertera et visa est, viribus. Divesque creatis, tecta novat collumque vulnus est, parvas. Faces illo pepulere tempus adest. Tendit flamma, ab opes virum sustinet, sidus sequendo urbis.
Iubar proles corpore raptos vero auctor imperium; sed et huic: manus caeli Lelegas tu lux. Verbis obstitit intus oblectamina fixis linguisque ausus sperare Echionides cornuaque tenent clausit possit. Omnia putatur. Praeteritae refert ausus; ferebant e primus lora nutat, vici quae mea ipse. Et iter nil spectatae vulnus haerentia iuste et exercebat, sui et.
Eurytus Hector, materna ipsumque ut Politen, nec, nate, ignari, vernum cohaesit sequitur. Vel mitis temploque vocatus, inque alis, oculos nomen non silvis corpore coniunx ne displicet illa. Crescunt non unus, vidit visa quantum inmiti flumina mortis facto sic: undique a alios vincula sunt iactata abdita! Suspenderat ego fuit tendit: luna, ante urbem Propoetides parte.
`}),X.addEventListener("input",function(){let i=this.value,n=e.search(i,5,{enrich:!0}),s=new Map;for(let r of n.flatMap(l=>l.result))s.has(r.href)||s.set(r.doc.href,r.doc);if(z.innerHTML="",z.classList.remove("search__suggestions--hidden"),s.size===0&&i){let r=document.createElement("div");r.innerHTML=`No results for "<strong>${i}</strong>"`,r.classList.add("search__no-results"),z.appendChild(r);return}for(let[r,l]of s){let h=document.createElement("a");h.href=r,h.classList.add("search__suggestion-item"),z.appendChild(h);let p=document.createElement("div");p.textContent=l.title,p.classList.add("search__suggestion-title"),h.appendChild(p);let g=document.createElement("div");if(g.textContent=l.description,g.classList.add("search__suggestion-description"),h.appendChild(g),z.childElementCount===5)break}})})();})();
//! Source: https://github.com/h-enk/doks/blob/master/assets/js/index.js
/*! Source: https://dev.to/shubhamprakash/trap-focus-using-javascript-6a3 */
//! Source: https://discourse.gohugo.io/t/range-length-or-last-element/3803/2

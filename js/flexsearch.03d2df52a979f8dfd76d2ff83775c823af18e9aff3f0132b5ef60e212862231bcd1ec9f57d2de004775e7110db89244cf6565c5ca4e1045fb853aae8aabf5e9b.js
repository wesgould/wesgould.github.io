(()=>{var ie=Object.create;var te=Object.defineProperty;var se=Object.getOwnPropertyDescriptor;var ae=Object.getOwnPropertyNames;var re=Object.getPrototypeOf,le=Object.prototype.hasOwnProperty;var ue=(e,o)=>()=>(o||e((o={exports:{}}).exports,o),o.exports);var he=(e,o,n,i)=>{if(o&&typeof o=="object"||typeof o=="function")for(let s of ae(o))!le.call(e,s)&&s!==n&&te(e,s,{get:()=>o[s],enumerable:!(i=se(o,s))||i.enumerable});return e};var de=(e,o,n)=>(n=e!=null?ie(re(e)):{},he(o||!e||!e.__esModule?te(n,"default",{value:e,enumerable:!0}):n,e));var oe=ue((exports,module)=>{(function _f(self){"use strict";try{module&&(self=module)}catch(e){}self._factory=_f;var t;function u(e){return typeof e!="undefined"?e:!0}function aa(e){let o=Array(e);for(let n=0;n<e;n++)o[n]=v();return o}function v(){return Object.create(null)}function ba(e,o){return o.length-e.length}function x(e){return typeof e=="string"}function C(e){return typeof e=="object"}function D(e){return typeof e=="function"}function ca(e,o){var n=da;if(e&&(o&&(e=E(e,o)),this.H&&(e=E(e,this.H)),this.J&&1<e.length&&(e=E(e,this.J)),n||n==="")){if(e=e.split(n),this.filter){o=this.filter,n=e.length;let i=[];for(let s=0,r=0;s<n;s++){let l=e[s];l&&!o[l]&&(i[r++]=l)}e=i}return e}return e}let da=/[\p{Z}\p{S}\p{P}\p{C}]+/u,ea=/[\u0300-\u036f]/g;function fa(e,o){let n=Object.keys(e),i=n.length,s=[],r="",l=0;for(let h=0,p,f;h<i;h++)p=n[h],(f=e[p])?(s[l++]=F(o?"(?!\\b)"+p+"(\\b|_)":p),s[l++]=f):r+=(r?"|":"")+p;return r&&(s[l++]=F(o?"(?!\\b)("+r+")(\\b|_)":"("+r+")"),s[l]=""),s}function E(e,o){for(let n=0,i=o.length;n<i&&(e=e.replace(o[n],o[n+1]),e);n+=2);return e}function F(e){return new RegExp(e,"g")}function ha(e){let o="",n="";for(let i=0,s=e.length,r;i<s;i++)(r=e[i])!==n&&(o+=n=r);return o}var ja={encode:ia,F:!1,G:""};function ia(e){return ca.call(this,(""+e).toLowerCase(),!1)}let ka={},G={};function la(e){I(e,"add"),I(e,"append"),I(e,"search"),I(e,"update"),I(e,"remove")}function I(e,o){e[o+"Async"]=function(){let n=this,i=arguments;var s=i[i.length-1];let r;return D(s)&&(r=s,delete i[i.length-1]),s=new Promise(function(l){setTimeout(function(){n.async=!0;let h=n[o].apply(n,i);n.async=!1,l(h)})}),r?(s.then(r),this):s}}function ma(e,o,n,i){let s=e.length,r=[],l,h,p=0;i&&(i=[]);for(let f=s-1;0<=f;f--){let g=e[f],q=g.length,w=v(),k=!l;for(let m=0;m<q;m++){let y=g[m],_=y.length;if(_)for(let B=0,R,A;B<_;B++)if(A=y[B],l){if(l[A]){if(!f){if(n)n--;else if(r[p++]=A,p===o)return r}(f||i)&&(w[A]=1),k=!0}if(i&&(R=(h[A]||0)+1,h[A]=R,R<s)){let H=i[R-2]||(i[R-2]=[]);H[H.length]=A}}else w[A]=1}if(i)l||(h=w);else if(!k)return[];l=w}if(i)for(let f=i.length-1,g,q;0<=f;f--){g=i[f],q=g.length;for(let w=0,k;w<q;w++)if(k=g[w],!l[k]){if(n)n--;else if(r[p++]=k,p===o)return r;l[k]=1}}return r}function na(e,o){let n=v(),i=v(),s=[];for(let r=0;r<e.length;r++)n[e[r]]=1;for(let r=0,l;r<o.length;r++){l=o[r];for(let h=0,p;h<l.length;h++)p=l[h],n[p]&&!i[p]&&(i[p]=1,s[s.length]=p)}return s}function J(e){this.l=e!==!0&&e,this.cache=v(),this.h=[]}function oa(e,o,n){C(e)&&(e=e.query);let i=this.cache.get(e);return i||(i=this.search(e,o,n),this.cache.set(e,i)),i}J.prototype.set=function(e,o){if(!this.cache[e]){var n=this.h.length;for(n===this.l?delete this.cache[this.h[n-1]]:n++,--n;0<n;n--)this.h[n]=this.h[n-1];this.h[0]=e}this.cache[e]=o},J.prototype.get=function(e){let o=this.cache[e];if(this.l&&o&&(e=this.h.indexOf(e))){let n=this.h[e-1];this.h[e-1]=this.h[e],this.h[e]=n}return o};let qa={memory:{charset:"latin:extra",D:3,B:4,m:!1},performance:{D:3,B:3,s:!1,context:{depth:2,D:1}},match:{charset:"latin:extra",G:"reverse"},score:{charset:"latin:advanced",D:20,B:3,context:{depth:3,D:9}},default:{}};function ra(e,o,n,i,s,r,l){setTimeout(function(){let h=e(n?n+"."+i:i,JSON.stringify(l));h&&h.then?h.then(function(){o.export(e,o,n,s,r+1)}):o.export(e,o,n,s,r+1)})}function K(e,o){if(!(this instanceof K))return new K(e);var n;if(e){x(e)?e=qa[e]:(n=e.preset)&&(e=Object.assign({},n[n],e)),n=e.charset;var i=e.lang;x(n)&&(n.indexOf(":")===-1&&(n+=":default"),n=G[n]),x(i)&&(i=ka[i])}else e={};let s,r,l=e.context||{};if(this.encode=e.encode||n&&n.encode||ia,this.register=o||v(),this.D=s=e.resolution||9,this.G=o=n&&n.G||e.tokenize||"strict",this.depth=o==="strict"&&l.depth,this.l=u(l.bidirectional),this.s=r=u(e.optimize),this.m=u(e.fastupdate),this.B=e.minlength||1,this.C=e.boost,this.map=r?aa(s):v(),this.A=s=l.resolution||1,this.h=r?aa(s):v(),this.F=n&&n.F||e.rtl,this.H=(o=e.matcher||i&&i.H)&&fa(o,!1),this.J=(o=e.stemmer||i&&i.J)&&fa(o,!0),n=o=e.filter||i&&i.filter){n=o,i=v();for(let h=0,p=n.length;h<p;h++)i[n[h]]=1;n=i}this.filter=n,this.cache=(o=e.cache)&&new J(o)}t=K.prototype,t.append=function(e,o){return this.add(e,o,!0)},t.add=function(e,o,n,i){if(o&&(e||e===0)){if(!i&&!n&&this.register[e])return this.update(e,o);if(o=this.encode(o),i=o.length){let f=v(),g=v(),q=this.depth,w=this.D;for(let k=0;k<i;k++){let m=o[this.F?i-1-k:k];var s=m.length;if(m&&s>=this.B&&(q||!g[m])){var r=L(w,i,k),l="";switch(this.G){case"full":if(2<s){for(r=0;r<s;r++)for(var h=s;h>r;h--)if(h-r>=this.B){var p=L(w,i,k,s,r);l=m.substring(r,h),M(this,g,l,p,e,n)}break}case"reverse":if(1<s){for(h=s-1;0<h;h--)l=m[h]+l,l.length>=this.B&&M(this,g,l,L(w,i,k,s,h),e,n);l=""}case"forward":if(1<s){for(h=0;h<s;h++)l+=m[h],l.length>=this.B&&M(this,g,l,r,e,n);break}default:if(this.C&&(r=Math.min(r/this.C(o,m,k)|0,w-1)),M(this,g,m,r,e,n),q&&1<i&&k<i-1){for(s=v(),l=this.A,r=m,h=Math.min(q+1,i-k),s[r]=1,p=1;p<h;p++)if((m=o[this.F?i-1-k-p:k+p])&&m.length>=this.B&&!s[m]){s[m]=1;let y=this.l&&m>r;M(this,f,y?r:m,L(l+(i/2>l?0:1),i,k,h-1,p-1),e,n,y?m:r)}}}}}this.m||(this.register[e]=1)}}return this};function L(e,o,n,i,s){return n&&1<e?o+(i||0)<=e?n+(s||0):(e-1)/(o+(i||0))*(n+(s||0))+1|0:0}function M(e,o,n,i,s,r,l){let h=l?e.h:e.map;(!o[n]||l&&!o[n][l])&&(e.s&&(h=h[i]),l?(o=o[n]||(o[n]=v()),o[l]=1,h=h[l]||(h[l]=v())):o[n]=1,h=h[n]||(h[n]=[]),e.s||(h=h[i]||(h[i]=[])),r&&h.includes(s)||(h[h.length]=s,e.m&&(e=e.register[s]||(e.register[s]=[]),e[e.length]=h)))}t.search=function(e,o,n){n||(!o&&C(e)?(n=e,e=n.query):C(o)&&(n=o));let i=[],s,r,l=0;if(n){e=n.query||e,o=n.limit,l=n.offset||0;var h=n.context;r=n.suggest}if(e&&(e=this.encode(""+e),s=e.length,1<s)){n=v();var p=[];for(let g=0,q=0,w;g<s;g++)if((w=e[g])&&w.length>=this.B&&!n[w])if(this.s||r||this.map[w])p[q++]=w,n[w]=1;else return i;e=p,s=e.length}if(!s)return i;o||(o=100),h=this.depth&&1<s&&h!==!1,n=0;let f;h?(f=e[0],n=1):1<s&&e.sort(ba);for(let g,q;n<s;n++){if(q=e[n],h?(g=sa(this,i,r,o,l,s===2,q,f),r&&g===!1&&i.length||(f=q)):g=sa(this,i,r,o,l,s===1,q),g)return g;if(r&&n===s-1){if(p=i.length,!p){if(h){h=0,n=-1;continue}return i}if(p===1)return ta(i[0],o,l)}}return ma(i,o,l,r)};function sa(e,o,n,i,s,r,l,h){let p=[],f=h?e.h:e.map;if(e.s||(f=ua(f,l,h,e.l)),f){let g=0,q=Math.min(f.length,h?e.A:e.D);for(let w=0,k=0,m,y;w<q&&!((m=f[w])&&(e.s&&(m=ua(m,l,h,e.l)),s&&m&&r&&(y=m.length,y<=s?(s-=y,m=null):(m=m.slice(s),s=0)),m&&(p[g++]=m,r&&(k+=m.length,k>=i))));w++);if(g){if(r)return ta(p,i,0);o[o.length]=p;return}}return!n&&p}function ta(e,o,n){return e=e.length===1?e[0]:[].concat.apply([],e),n||e.length>o?e.slice(n,n+o):e}function ua(e,o,n,i){return n?(i=i&&o>n,e=(e=e[i?o:n])&&e[i?n:o]):e=e[o],e}t.contain=function(e){return!!this.register[e]},t.update=function(e,o){return this.remove(e).add(e,o)},t.remove=function(e,o){let n=this.register[e];if(n){if(this.m)for(let i=0,s;i<n.length;i++)s=n[i],s.splice(s.indexOf(e),1);else N(this.map,e,this.D,this.s),this.depth&&N(this.h,e,this.A,this.s);if(o||delete this.register[e],this.cache){o=this.cache;for(let i=0,s,r;i<o.h.length;i++)r=o.h[i],s=o.cache[r],s.includes(e)&&(o.h.splice(i--,1),delete o.cache[r])}}return this};function N(e,o,n,i,s){let r=0;if(e.constructor===Array)if(s)o=e.indexOf(o),o!==-1?1<e.length&&(e.splice(o,1),r++):r++;else{s=Math.min(e.length,n);for(let l=0,h;l<s;l++)(h=e[l])&&(r=N(h,o,n,i,s),i||r||delete e[l])}else for(let l in e)(r=N(e[l],o,n,i,s))||delete e[l];return r}t.searchCache=oa,t.export=function(e,o,n,i,s){let r,l;switch(s||(s=0)){case 0:if(r="reg",this.m){l=v();for(let h in this.register)l[h]=1}else l=this.register;break;case 1:r="cfg",l={doc:0,opt:this.s?1:0};break;case 2:r="map",l=this.map;break;case 3:r="ctx",l=this.h;break;default:return}return ra(e,o||this,n,r,i,s,l),!0},t.import=function(e,o){if(o)switch(x(o)&&(o=JSON.parse(o)),e){case"cfg":this.s=!!o.opt;break;case"reg":this.m=!1,this.register=o;break;case"map":this.map=o;break;case"ctx":this.h=o}},la(K.prototype);function va(e){e=e.data;var o=self._index;let n=e.args;var i=e.task;switch(i){case"init":i=e.options||{},e=e.factory,o=i.encode,i.cache=!1,o&&o.indexOf("function")===0&&(i.encode=Function("return "+o)()),e?(Function("return "+e)()(self),self._index=new self.FlexSearch.Index(i),delete self.FlexSearch):self._index=new K(i);break;default:e=e.id,o=o[i].apply(o,n),postMessage(i==="search"?{id:e,msg:o}:{id:e})}}let wa=0;function O(e){if(!(this instanceof O))return new O(e);var o;e?D(o=e.encode)&&(e.encode=o.toString()):e={},(o=(self||window)._factory)&&(o=o.toString());let n=typeof window=="undefined"&&self.exports,i=this;this.o=xa(o,n,e.worker),this.h=v(),this.o&&(n?this.o.on("message",function(s){i.h[s.id](s.msg),delete i.h[s.id]}):this.o.onmessage=function(s){s=s.data,i.h[s.id](s.msg),delete i.h[s.id]},this.o.postMessage({task:"init",factory:o,options:e}))}P("add"),P("append"),P("search"),P("update"),P("remove");function P(e){O.prototype[e]=O.prototype[e+"Async"]=function(){let o=this,n=[].slice.call(arguments);var i=n[n.length-1];let s;return D(i)&&(s=i,n.splice(n.length-1,1)),i=new Promise(function(r){setTimeout(function(){o.h[++wa]=r,o.o.postMessage({task:e,id:wa,args:n})})}),s?(i.then(s),this):i}}function xa(a,b,c){let d;try{d=b?eval('new (require("worker_threads")["Worker"])("../dist/node/node.js")'):a?new Worker(URL.createObjectURL(new Blob(["onmessage="+va.toString()],{type:"text/javascript"}))):new Worker(x(c)?c:"worker/worker.js",{type:"module"})}catch(e){}return d}function Q(e){if(!(this instanceof Q))return new Q(e);var o=e.document||e.doc||e,n;this.K=[],this.h=[],this.A=[],this.register=v(),this.key=(n=o.key||o.id)&&S(n,this.A)||"id",this.m=u(e.fastupdate),this.C=(n=o.store)&&n!==!0&&[],this.store=n&&v(),this.I=(n=o.tag)&&S(n,this.A),this.l=n&&v(),this.cache=(n=e.cache)&&new J(n),e.cache=!1,this.o=e.worker,this.async=!1,n=v();let i=o.index||o.field||o;x(i)&&(i=[i]);for(let s=0,r,l;s<i.length;s++)r=i[s],x(r)||(l=r,r=r.field),l=C(l)?Object.assign({},e,l):e,this.o&&(n[r]=new O(l),n[r].o||(this.o=!1)),this.o||(n[r]=new K(l,this.register)),this.K[s]=S(r,this.A),this.h[s]=r;if(this.C)for(e=o.store,x(e)&&(e=[e]),o=0;o<e.length;o++)this.C[o]=S(e[o],this.A);this.index=n}function S(e,o){let n=e.split(":"),i=0;for(let s=0;s<n.length;s++)e=n[s],0<=e.indexOf("[]")&&(e=e.substring(0,e.length-2))&&(o[i]=!0),e&&(n[i++]=e);return i<n.length&&(n.length=i),1<i?n:n[0]}function T(e,o){if(x(o))e=e[o];else for(let n=0;e&&n<o.length;n++)e=e[o[n]];return e}function U(e,o,n,i,s){if(e=e[s],i===n.length-1)o[s]=e;else if(e)if(e.constructor===Array)for(o=o[s]=Array(e.length),s=0;s<e.length;s++)U(e,o,n,i,s);else o=o[s]||(o[s]=v()),s=n[++i],U(e,o,n,i,s)}function V(e,o,n,i,s,r,l,h){if(e=e[l])if(i===o.length-1){if(e.constructor===Array){if(n[i]){for(o=0;o<e.length;o++)s.add(r,e[o],!0,!0);return}e=e.join(" ")}s.add(r,e,h,!0)}else if(e.constructor===Array)for(l=0;l<e.length;l++)V(e,o,n,i,s,r,l,h);else l=o[++i],V(e,o,n,i,s,r,l,h)}t=Q.prototype,t.add=function(e,o,n){if(C(e)&&(o=e,e=T(o,this.key)),o&&(e||e===0)){if(!n&&this.register[e])return this.update(e,o);for(let i=0,s,r;i<this.h.length;i++)r=this.h[i],s=this.K[i],x(s)&&(s=[s]),V(o,s,this.A,0,this.index[r],e,s[0],n);if(this.I){let i=T(o,this.I),s=v();x(i)&&(i=[i]);for(let r=0,l,h;r<i.length;r++)if(l=i[r],!s[l]&&(s[l]=1,h=this.l[l]||(this.l[l]=[]),!n||!h.includes(e))&&(h[h.length]=e,this.m)){let p=this.register[e]||(this.register[e]=[]);p[p.length]=h}}if(this.store&&(!n||!this.store[e])){let i;if(this.C){i=v();for(let s=0,r;s<this.C.length;s++)r=this.C[s],x(r)?i[r]=o[r]:U(o,i,r,0,r[0])}this.store[e]=i||o}}return this},t.append=function(e,o){return this.add(e,o,!0)},t.update=function(e,o){return this.remove(e).add(e,o)},t.remove=function(e){if(C(e)&&(e=T(e,this.key)),this.register[e]){for(var o=0;o<this.h.length&&(this.index[this.h[o]].remove(e,!this.o),!this.m);o++);if(this.I&&!this.m)for(let n in this.l){o=this.l[n];let i=o.indexOf(e);i!==-1&&(1<o.length?o.splice(i,1):delete this.l[n])}this.store&&delete this.store[e],delete this.register[e]}return this},t.search=function(e,o,n,i){n||(!o&&C(e)?(n=e,e=""):C(o)&&(n=o,o=0));let s=[],r=[],l,h,p,f,g,q,w=0;if(n)if(n.constructor===Array)p=n,n=null;else{if(e=n.query||e,p=(l=n.pluck)||n.index||n.field,f=n.tag,h=this.store&&n.enrich,g=n.bool==="and",o=n.limit||o||100,q=n.offset||0,f&&(x(f)&&(f=[f]),!e)){for(let m=0,y;m<f.length;m++)(y=ya.call(this,f[m],o,q,h))&&(s[s.length]=y,w++);return w?s:[]}x(p)&&(p=[p])}p||(p=this.h),g=g&&(1<p.length||f&&1<f.length);let k=!i&&(this.o||this.async)&&[];for(let m=0,y,_,B;m<p.length;m++){let R;if(_=p[m],x(_)||(R=_,_=R.field,e=R.query||e,o=R.limit||o),k)k[m]=this.index[_].searchAsync(e,o,R||n);else{if(i?y=i[m]:y=this.index[_].search(e,o,R||n),B=y&&y.length,f&&B){let A=[],H=0;g&&(A[0]=[y]);for(let $=0,ee,j;$<f.length;$++)ee=f[$],(B=(j=this.l[ee])&&j.length)&&(H++,A[A.length]=g?[j]:j);H&&(y=g?ma(A,o||100,q||0):na(y,A),B=y.length)}if(B)r[w]=_,s[w++]=y;else if(g)return[]}}if(k){let m=this;return new Promise(function(y){Promise.all(k).then(function(_){y(m.search(e,o,n,_))})})}if(!w)return[];if(l&&(!h||!this.store))return s[0];for(let m=0,y;m<r.length;m++){if(y=s[m],y.length&&h&&(y=za.call(this,y)),l)return y;s[m]={field:r[m],result:y}}return s};function ya(e,o,n,i){let s=this.l[e],r=s&&s.length-n;if(r&&0<r)return(r>o||n)&&(s=s.slice(n,n+o)),i&&(s=za.call(this,s)),{tag:e,result:s}}function za(e){let o=Array(e.length);for(let n=0,i;n<e.length;n++)i=e[n],o[n]={id:i,doc:this.store[i]};return o}t.contain=function(e){return!!this.register[e]},t.get=function(e){return this.store[e]},t.set=function(e,o){return this.store[e]=o,this},t.searchCache=oa,t.export=function(e,o,n,i,s){if(s||(s=0),i||(i=0),i<this.h.length){let r=this.h[i],l=this.index[r];o=this,setTimeout(function(){l.export(e,o,s?r:"",i,s++)||(i++,s=1,o.export(e,o,r,i,s))})}else{let r,l;switch(s){case 1:r="tag",l=this.l;break;case 2:r="store",l=this.store;break;default:return}ra(e,this,n,r,i,s,l)}},t.import=function(e,o){if(o)switch(x(o)&&(o=JSON.parse(o)),e){case"tag":this.l=o;break;case"reg":this.m=!1,this.register=o;for(let i=0,s;i<this.h.length;i++)s=this.index[this.h[i]],s.register=o,s.m=!1;break;case"store":this.store=o;break;default:e=e.split(".");let n=e[0];e=e[1],n&&e&&this.index[n].import(e,o)}},la(Q.prototype);var Ba={encode:Aa,F:!1,G:""};let Ca=[F("[\xE0\xE1\xE2\xE3\xE4\xE5]"),"a",F("[\xE8\xE9\xEA\xEB]"),"e",F("[\xEC\xED\xEE\xEF]"),"i",F("[\xF2\xF3\xF4\xF5\xF6\u0151]"),"o",F("[\xF9\xFA\xFB\xFC\u0171]"),"u",F("[\xFD\u0177\xFF]"),"y",F("\xF1"),"n",F("[\xE7c]"),"k",F("\xDF"),"s",F(" & ")," and "];function Aa(e){var o=e=""+e;return o.normalize&&(o=o.normalize("NFD").replace(ea,"")),ca.call(this,o.toLowerCase(),!e.normalize&&Ca)}var Ea={encode:Da,F:!1,G:"strict"};let Fa=/[^a-z0-9]+/,Ga={b:"p",v:"f",w:"f",z:"s",x:"s",\u00DF:"s",d:"t",n:"m",c:"k",g:"k",j:"k",q:"k",i:"e",y:"e",u:"o"};function Da(e){e=Aa.call(this,e).join(" ");let o=[];if(e){let n=e.split(Fa),i=n.length;for(let s=0,r,l=0;s<i;s++)if((e=n[s])&&(!this.filter||!this.filter[e])){r=e[0];let h=Ga[r]||r,p=h;for(let f=1;f<e.length;f++){r=e[f];let g=Ga[r]||r;g&&g!==p&&(h+=g,p=g)}o[l++]=h}}return o}var Ia={encode:Ha,F:!1,G:""};let Ja=[F("ae"),"a",F("oe"),"o",F("sh"),"s",F("th"),"t",F("ph"),"f",F("pf"),"f",F("(?![aeo])h(?![aeo])"),"",F("(?!^[aeo])h(?!^[aeo])"),""];function Ha(e,o){return e&&(e=Da.call(this,e).join(" "),2<e.length&&(e=E(e,Ja)),o||(1<e.length&&(e=ha(e)),e&&(e=e.split(" ")))),e||[]}var La={encode:Ka,F:!1,G:""};let Ma=F("(?!\\b)[aeo]");function Ka(e){return e&&(e=Ha.call(this,e,!0),1<e.length&&(e=e.replace(Ma,"")),1<e.length&&(e=ha(e)),e&&(e=e.split(" "))),e||[]}G["latin:default"]=ja,G["latin:simple"]=Ba,G["latin:balance"]=Ea,G["latin:advanced"]=Ia,G["latin:extra"]=La;let W=self,Y,Z={Index:K,Document:Q,Worker:O,registerCharset:function(e,o){G[e]=o},registerLanguage:function(e,o){ka[e]=o}};(Y=W.define)&&Y.amd?Y([],function(){return Z}):W.exports?W.exports=Z:W.FlexSearch=Z})(exports)});var ne=de(oe());var{Document:ce}=ne.default,X=document.getElementById("search__text"),z=document.getElementById("search__suggestions");X!==null&&document.addEventListener("keydown",e=>{e.ctrlKey&&e.key==="/"?(e.preventDefault(),X.focus()):e.key==="Escape"&&(X.blur(),z.classList.add("search__suggestions--hidden"))});document.addEventListener("click",e=>{z.contains(e.target)||z.classList.add("search__suggestions--hidden")});document.addEventListener("keydown",e=>{if(z.classList.contains("search__suggestions--hidden"))return;let n=[...z.querySelectorAll("a")];if(n.length===0)return;let i=n.indexOf(document.activeElement);if(e.key==="ArrowDown"){e.preventDefault();let s=i+1<n.length?i+1:i;n[s].focus()}else e.key==="ArrowUp"&&(e.preventDefault(),nextIndex=i>0?i-1:0,n[nextIndex].focus())});(function(){let e=new ce({tokenize:"forward",cache:100,document:{id:"id",store:["href","title","description"],index:["title","description","content"]}});e.add({id:0,href:"/recipes/italian-sausage-tortellini-soup/",title:"Italian Sausage Tortellini Soup",description:`My wife found this recipe on instagram or someplace and has made it a few times. It&rsquo;s fantastic and its relatively easy to make. The original recipe calls for gnocchi, but my wife, who doesn&rsquo;t like the texture of gnocchi, &lsquo;accidentally&rsquo; grabbed 3 cheese Tortellini the first time she made this and thats the way we tend to make it.
So, you can make it with Tortellini or gnocci.
Ingredients# 1 lb ground mild Italian sausage 8 oz bag of spinach, roughly chopped (or hand-shredded) 2 tbsp minced garlic 2 cups low-sodium chicken broth 2 cups half-and-half (or heavy cream) 3 oz grated Parmesan 1 package of 3 cheese Tortellini 1 tbsp tomato paste 1 tsp olive oil 1 tsp salt 1 tsp pepper 1 tsp Italian seasoning 1 tsp oregano 1 tsp parsley 1 tsp crushed red pepper flakes 1 tsp onion powder 1 tsp garlic powder Instructions# 1. Heat oil &amp; garlic# Add 1 tsp olive oil to a large pot or Dutch oven over medium heat. Add the minced garlic and cook until fragrant.
`,content:`My wife found this recipe on instagram or someplace and has made it a few times. It&rsquo;s fantastic and its relatively easy to make. The original recipe calls for gnocchi, but my wife, who doesn&rsquo;t like the texture of gnocchi, &lsquo;accidentally&rsquo; grabbed 3 cheese Tortellini the first time she made this and thats the way we tend to make it.
So, you can make it with Tortellini or gnocci.
Ingredients# 1 lb ground mild Italian sausage 8 oz bag of spinach, roughly chopped (or hand-shredded) 2 tbsp minced garlic 2 cups low-sodium chicken broth 2 cups half-and-half (or heavy cream) 3 oz grated Parmesan 1 package of 3 cheese Tortellini 1 tbsp tomato paste 1 tsp olive oil 1 tsp salt 1 tsp pepper 1 tsp Italian seasoning 1 tsp oregano 1 tsp parsley 1 tsp crushed red pepper flakes 1 tsp onion powder 1 tsp garlic powder Instructions# 1. Heat oil &amp; garlic# Add 1 tsp olive oil to a large pot or Dutch oven over medium heat. Add the minced garlic and cook until fragrant.
2. Cook sausage# Add Italian sausage and cook completely, breaking it up as it browns. Then add all seasonings (salt, pepper, Italian seasoning, oregano, parsley, crushed red pepper flakes, onion powder, garlic powder) plus the tomato paste. Stir to combine.
3. Add broth &amp; gnocchi# Pour in chicken broth and bring to a boil. Add the gnocchi and cook for 3\u20135 minutes until they float to the surface.
4. Add cream, Parmesan &amp; spinach# Reduce heat to low and add the half-and-half or cream. Add grated Parmesan and stir until melted. Finally, stir in the shredded spinach until wilted.
5. Simmer &amp; serve# Let everything simmer for about 10 minutes until slightly thickened, then serve hot.
Makes enough for 4 people.
`}).add({id:1,href:"/projects/voxdex-ai-transcription/",title:"AI-Powered Podcast Transcription System",description:`AI Auto Transcription System# Introduction# About two years ago, right as the LLM craze was really popping off, I stumbled on WhisperAI&ndash;a tool released by OpenAI that let you transcribe audio (voice-to-text). I actually uncovered it when I was looking at a way to dictate some of my personal notes in Neovim (vimwiki). I found nerd-dictation [https://github.com/ideasman42/nerd-dictation] which was super amazing, but didn&rsquo;t work on my macbook and died on me as linux began the march away from x11 to wayland. Nerd-dictation required some of the security&hellip;&ldquo;openness&rdquo; that X11 afforded. I switched to wayland to get something working on my Pop_OS! machine, and when I did that, I lost my ability to use nerd-dictation.
`,content:`AI Auto Transcription System# Introduction# About two years ago, right as the LLM craze was really popping off, I stumbled on WhisperAI&ndash;a tool released by OpenAI that let you transcribe audio (voice-to-text). I actually uncovered it when I was looking at a way to dictate some of my personal notes in Neovim (vimwiki). I found nerd-dictation [https://github.com/ideasman42/nerd-dictation] which was super amazing, but didn&rsquo;t work on my macbook and died on me as linux began the march away from x11 to wayland. Nerd-dictation required some of the security&hellip;&ldquo;openness&rdquo; that X11 afforded. I switched to wayland to get something working on my Pop_OS! machine, and when I did that, I lost my ability to use nerd-dictation.
That&rsquo;s was around the time I heard about WhisperCPP on a TWiT podcast. It&rsquo;s a fantastic project by Georgi Gerganov, and you can find information on it here https://github.com/ggerganov/whisper.cpp. As an aside, Georgi Gerganov is the same guy of gguf fame. A leader in AI development/AI tool development / open source. He&rsquo;s ported the OpenAI Whisper code to C++ and even added support for the Core ML cores on Apple Silicon. His implementation is the fastest I&rsquo;ve seen so far (especially when taking advantage of the ML cores on the M-series chips). When I started playing around with it I realized it could do much more than dictation. Which gave me the idea for transcribing the podcasts I listen to.
Why?# As an avid listener of the TWiT (This Week in Tech) network for the last 18 years 20 years, I often find myself trying to remember what someone said, how something was phrased, or &ldquo;what was it that they said about privacy regulation?&rdquo; Obviously, it&rsquo;s pretty tough to remember the episode (or even the right show), and it&rsquo;s impractical to re-listen to thousands of hours of audio. So I had the idea of transcribing the audio and video so that it&rsquo;s easily searchable text.
I want to say I was on paternity leave when I wrote the first very kludgey version of this tool. It consisted of a couple of poorly written python scripts. Some of which I copy and pasted from stackoverflow and various forums on the internet. It was glued together with bash scripts and cron jobs&hellip;and as you can imagine it was incredibly fragile. Every time I rebooted the machine I had to go relaunch things. I tried to automate this but it was so finicky that I often needed to ssh into the machine and do it manually.
I could blame it on sleep deprivation but my lack of software development skill was probably the biggest culprit. It was literally hacked together in the truest sense of the word.
v2.0# Fast forward to now. The AI hype cycle is at peak hype and billions of VC dollars have been invested in LLMs and other AI tech.
Then here I was, letting my AI tool languish on my server, not taking advantage of the progress of the last few years.
There were a few reasons I decided to dust this off:
I wanted to reorganize my server rack. I wanted to virtualise my truenas server (my old gaming machine). To do that, I needed to free up the PCI slot on my proxmox server that my old GPU was sitting it. That was the GPU I was using to do the transcripts. Due to a limitation of the motherboard on my proxmox server, I can only pass through that PCI slot, and I need it for my HBA card to give me enough SATA connections for my truenas hard drives. Ultimately, this would let me spin down that server and save a few bucks on electricity (or, lets be honest, use it for something else and not save electricity). ROCm finally doesn&rsquo;t suck butt, and I figured I could run the transcription process on my desktop now using my Radeon 6900xt. It would let me use a much larger (more accurate model) and would be faster than my macbook air or the 1070ti in the server. Which btw, I bought secondhand from my brother-in-law specifically to do the transcription work since it was so painful to use ROCm back then. I literally spent $175 to get an nvidia card to avoid the pain that was ROCm. The aforementioned fragileness of my old setup resulted in it needing more manual intervention than I had time to provide so it just languished, broken, for a few months. Part of the old system required me to manually go in and &ldquo;identify&rdquo; speakers then find/replace the &ldquo;SPEAKER_01&rdquo; with &ldquo;Leo Laporte&rdquo; etc. I knew that an LLM should be able to do this. Claude Code could, theoretically, fill in the programming skill gap that plagued my previous attempt. Technical Solution# Core Components# flowchart TD A[RSS Feeds] --> B[Audio Downloader] B --> C[Audio Storage] C --> D1[TranscriptionWhisper/WhisperX] C --> D2[Diarizationpyannote.audio] D1 --> E[Merge Segments] D2 --> E E --> F[LLM Speaker ID] F --> G[Export FormatsTXT/JSON/SRT] G --> H[File Management] Key Technologies# Transcription Engine: OpenAI Whisper Speaker Diarization: WhisperX + pyannote.audio LLM Integration: Optional GPT/Claude integration Infrastructure: Python 3.10+, AMD ROCm/NVIDIA CUDA support Unique Features# Three-Tier Transcript System# It generates three distinct versions of each transcript to serve different use cases:
1. Raw Transcription Output# Obviously a purely raw output is nice to have but its not the most &lsquo;usable&rsquo;.
Welcome to the Intelligent Machines podcast. Today we&#39;re diving deep into the world of artificial intelligence and machine learning. I&#39;m excited to have our guest here to discuss the latest developments in neural networks and their applications in real-world scenarios. The field has been advancing at an incredible pace, especially with the emergence of large language models and their impact on various industries. 2. Speaker-Diarized Version# The diarized version, aka the version with the speakers being differentiated, is even better.
SPEAKER_01: Welcome to the Intelligent Machines podcast. Today we&#39;re diving deep into the world of artificial intelligence and machine learning. SPEAKER_01: I&#39;m excited to have our guest here to discuss the latest developments in neural networks and their applications in real-world scenarios. SPEAKER_02: Thanks for having me on the show. SPEAKER_01: The field has been advancing at an incredible pace, especially with the emergence of large language models and their impact on various industries. SPEAKER_02: Absolutely, and what&#39;s particularly fascinating is how these models are being integrated into everyday applications. 3. LLM-Enhanced with Real Speaker Names# This is the real piece-de-resistance. Like I said before, I use to have to go into the diarized version (like you see above) and use context clues to figure out who SPEAKER_01 is. That was a very time consuming manual task and honestly I never got around to doing it for the thousands of episodes I transcribed and diarized.
But this new version uses an LLM to identify the speakers and make the changes. This part is trickier than it seems because the diarization part isn&rsquo;t the most accurate. It can get confused and think there are more speakers than their are because of Ads or things like overlapping conversation (which happens all of the time on podcasts and real life). Pyannote can struggle distinguishing speakers in those cases. But if you prompt an LLM well enough&hellip;
Leo Laporte: Welcome to the Intelligent Machines podcast. Today we&#39;re diving deep into the world of artificial intelligence and machine learning. Leo Laporte: I&#39;m excited to have our guest here to discuss the latest developments in neural networks and their applications in real-world scenarios. Dr. Sarah Chen: Thanks for having me on the show. Leo Laporte: The field has been advancing at an incredible pace, especially with the emergence of large language models and their impact on various industries. Dr. Sarah Chen: Absolutely, and what&#39;s particularly fascinating is how these models are being integrated into everyday applications. Automation Capabilities# RSS Feed Batch Processing# VoxDex can process multiple podcast feeds automatically by configuring RSS sources:
# config.yaml rss_feeds: - name: &#34;TWiT This Week in Tech&#34; url: &#34;https://feeds.twit.tv/twit.xml&#34; enabled: true - name: &#34;AI Podcast&#34; url: &#34;https://feeds.example.com/ai-podcast.xml&#34; enabled: true processing: max_episodes_per_run: 5 check_interval_hours: 6 # RSS processing example def process_rss_feeds(config): for feed in config[&#39;rss_feeds&#39;]: if feed[&#39;enabled&#39;]: episodes = fetch_new_episodes(feed[&#39;url&#39;]) for episode in episodes[:config[&#39;processing&#39;][&#39;max_episodes_per_run&#39;]]: download_and_process(episode) Configurable File Retention# Automatic cleanup based on age and storage limits:
# File retention configuration retention: audio_files: keep_days: 30 max_size_gb: 100 transcripts: keep_days: 365 backup_to_cloud: true temp_files: cleanup_immediately: true # Cleanup implementation def cleanup_old_files(): # Remove audio files older than 30 days for file in get_audio_files(): if file.age_days &gt; config.retention.audio_files.keep_days: file.delete() # Archive old transcripts to cloud storage for transcript in get_old_transcripts(): if config.retention.transcripts.backup_to_cloud: upload_to_cloud(transcript) Multiple Output Formats# Generate TXT, JSON, and SRT simultaneously:
# Export to multiple formats def export_transcript(segments, episode_metadata): base_filename = f&#34;{episode_metadata[&#39;show&#39;]}_{episode_metadata[&#39;date&#39;]}&#34; # Plain text format with open(f&#34;{base_filename}.txt&#34;, &#34;w&#34;) as f: for segment in segments: f.write(f&#34;{segment[&#39;speaker&#39;]}: {segment[&#39;text&#39;]}\\n\\n&#34;) # JSON format with metadata json_output = { &#34;metadata&#34;: episode_metadata, &#34;segments&#34;: [ { &#34;start_time&#34;: seg[&#39;start&#39;], &#34;end_time&#34;: seg[&#39;end&#39;], &#34;speaker&#34;: seg[&#39;speaker&#39;], &#34;text&#34;: seg[&#39;text&#39;] } for seg in segments ] } with open(f&#34;{base_filename}.json&#34;, &#34;w&#34;) as f: json.dump(json_output, f, indent=2) # SRT subtitle format with open(f&#34;{base_filename}.srt&#34;, &#34;w&#34;) as f: for i, segment in enumerate(segments, 1): f.write(f&#34;{i}\\n&#34;) f.write(f&#34;{format_time(segment[&#39;start&#39;])} --&gt; {format_time(segment[&#39;end&#39;])}\\n&#34;) f.write(f&#34;{segment[&#39;speaker&#39;]}: {segment[&#39;text&#39;]}\\n\\n&#34;) Sample Outputs# TXT Format (intelligent_machines_2024-10-05.txt):
Leo Laporte: Welcome to the Intelligent Machines podcast. Today we&#39;re diving deep into the world of artificial intelligence and machine learning. Leo Laporte: I&#39;m excited to have our guest here to discuss the latest developments in neural networks and their applications in real-world scenarios. Dr. Sarah Chen: Thanks for having me on the show. Leo Laporte: The field has been advancing at an incredible pace, especially with the emergence of large language models and their impact on various industries. Dr. Sarah Chen: Absolutely, and what&#39;s particularly fascinating is how these models are being integrated into everyday applications. JSON Format (intelligent_machines_2024-10-05.json):
{ &#34;metadata&#34;: { &#34;show&#34;: &#34;Intelligent Machines&#34;, &#34;date&#34;: &#34;2024-10-05&#34;, &#34;episode_title&#34;: &#34;Neural Networks in Practice&#34;, &#34;duration&#34;: &#34;3600&#34;, &#34;file_size&#34;: &#34;156MB&#34; }, &#34;segments&#34;: [ { &#34;start_time&#34;: &#34;00:00:00&#34;, &#34;end_time&#34;: &#34;00:00:08&#34;, &#34;speaker&#34;: &#34;Leo Laporte&#34;, &#34;text&#34;: &#34;Welcome to the Intelligent Machines podcast. Today we&#39;re diving deep into the world of artificial intelligence and machine learning.&#34; }, { &#34;start_time&#34;: &#34;00:00:08&#34;, &#34;end_time&#34;: &#34;00:00:16&#34;, &#34;speaker&#34;: &#34;Leo Laporte&#34;, &#34;text&#34;: &#34;I&#39;m excited to have our guest here to discuss the latest developments in neural networks and their applications in real-world scenarios.&#34; }, { &#34;start_time&#34;: &#34;00:00:16&#34;, &#34;end_time&#34;: &#34;00:00:18&#34;, &#34;speaker&#34;: &#34;Dr. Sarah Chen&#34;, &#34;text&#34;: &#34;Thanks for having me on the show.&#34; }, { &#34;start_time&#34;: &#34;00:00:18&#34;, &#34;end_time&#34;: &#34;00:00:28&#34;, &#34;speaker&#34;: &#34;Leo Laporte&#34;, &#34;text&#34;: &#34;The field has been advancing at an incredible pace, especially with the emergence of large language models and their impact on various industries.&#34; }, { &#34;start_time&#34;: &#34;00:00:28&#34;, &#34;end_time&#34;: &#34;00:00:35&#34;, &#34;speaker&#34;: &#34;Dr. Sarah Chen&#34;, &#34;text&#34;: &#34;Absolutely, and what&#39;s particularly fascinating is how these models are being integrated into everyday applications.&#34; } ] } SRT Format (intelligent_machines_2024-10-05.srt):
1 00:00:00,000 --&gt; 00:00:08,000 Leo Laporte: Welcome to the Intelligent Machines podcast. Today we&#39;re diving deep into the world of artificial intelligence and machine learning. 2 00:00:08,000 --&gt; 00:00:16,000 Leo Laporte: I&#39;m excited to have our guest here to discuss the latest developments in neural networks and their applications in real-world scenarios. 3 00:00:16,000 --&gt; 00:00:18,000 Dr. Sarah Chen: Thanks for having me on the show. 4 00:00:18,000 --&gt; 00:00:28,000 Leo Laporte: The field has been advancing at an incredible pace, especially with the emergence of large language models and their impact on various industries. 5 00:00:28,000 --&gt; 00:00:35,000 Dr. Sarah Chen: Absolutely, and what&#39;s particularly fascinating is how these models are being integrated into everyday applications. Technical Challenges &amp; Solutions# Speaker diarization accuracy: The pyannote project has made some great strides and is impressive software. There is a huge active community on huggingface.com supporting it. That said, it still struggles a bit at identifying unique speakers when
you don&rsquo;t tell it exactly how many voices it should expect (and that&rsquo;s not as straightforward as it seems because you have ads, they play clips of audio, etc.) people over-talk each other which happens all the time during normal conversation and especially on conversational podcasts where latency and your typical zoom lag come into play. Its good enough though, and when you combine it with an LLM you can pretty accurately identify who is saying what.
One thing I had to prompt the LLM with was letting it know that the same person could be SPEAKER_01 and SPEAKER_05 because of the imprecise nature of the diarization from pyannote. The LLM, initially, would only assign a name to one of those, which makes sense if you are an LLM and assume each &ldquo;SPEAKER_nn&rdquo; is unique. So, I had to let it know that different &ldquo;SPEAKER&quot;s could be the same person and to use further context clues.
GPU optimization: I have another post about getting ROCm working here. It was pretty seamless to get the whisperX code working locally with my GPU. I haven&rsquo;t done much in terms of optimizing. I changed to use the largest (English only) model which my GPU can handle. There is probably room for more tuning here. For now, I don&rsquo;t run it often enough and only do a handful of podcasts so if this isn&rsquo;t even going to move the needle on my electric bill.
LLM integration challenges: I wasn&rsquo;t having much luck with the smaller (cheaper) LLMs like gpt5nano. I could probably spend a little more time tuning my prompts or tweaking the sampling code to make it work better for me, but I decided to just throw the bigger model at it and call it a day. Its not too expensive at this point. I need to work out the math but its probably not much more than a small coffee at Starbucks (per month).
File management and processing pipeline: I added some configurable features to prune the downloaded podcasts. One of the issues I ran into with my old clunky setup was running out of disk space because I downloaded, and never deleted, ALL of the episodes on my limited VM filesystem.
# Retention configuration for managing storage retention: # Audio files (raw downloaded episodes) audio_files: keep_days: 7 # Delete after 1 week max_size_gb: 50 # Clean oldest when storage exceeds limit # Transcript files (.txt, .json, .srt) transcripts: keep_days: 365 # Keep transcripts for 1 year backup_before_delete: true backup_location: &#34;/path/to/backup&#34; # Temporary processing files temp_files: cleanup_immediately: true keep_on_error: true # Keep temp files if processing fails # Failed processing attempts failed_downloads: retry_after_days: 3 max_retries: 3 delete_after_days: 30 Next Steps# So whats next for this project? My plan is to run it for a month then check the results and performance. I am also planning on looking into a better way to index this for search or more meaningful use. Having raw text files is great because there are so many simple tools or utilities that can help dig through them (fzf, rg, telescope in neovim), but there is value in putting it into a database or Elasticsearch. There are some text analysis tools that could be fun to throw at it too (eg sentiment analysis, theme extraction, etc).
Other things I want to do at some point:
See if I can get a local LLM to perform as well as ChatGPT 5. It would be pretty cool to do all of this locally. (Though the cost to do this with the openai api is literally pennies). Create a utility script that can take my OLD diarized podcasts and run it through the LLM enhancement tool. I dont think I want to spend the time or energy re-processing thousands of old shows. The diarization was &ldquo;ok&rdquo; and I think the LLM enhancement piece would clean it up so its usable. Clean up my GitHub repository: https://github.com/wesgould/voxdex Getting started guide `}).add({id:2,href:"/recipes/roasted-tomato-burrata-dip/",title:"Roasted Tomato & Burrata Dip with Garlic Crostinis",description:`Stoney made this during his night to cook during the OBX trip in 2025. Ripe bursted cherry tomatoes, stir in burrata or stracciatella, and scoop it all up with crispy garlic crostinis.
Ingredients# # For the dip 2 cups cherry or grape tomatoes 2 Tbsp olive oil (or enough to coat the baking dish) \xBD tsp salt (or to taste) \xBC tsp freshly ground black pepper Pinch of red pepper flakes (optional, for heat) 1\u20132 cloves garlic, minced 1 Tbsp chopped fresh parsley 1 tsp dried oregano (or 1 Tbsp fresh, chopped) 1 burrata (or equivalent amount of stracciatella / burrata filling) Handful of fresh basil leaves, torn 2 Tbsp freshly grated Parmesan (more for serving, optional) # For the garlic crostinis 1 French baguette (or similar crusty bread) 2\u20133 Tbsp butter, softened 1 clove garlic, minced Pinch of salt 1 Tbsp chopped parsley 1 Tbsp grated Parmesan (optional) Instructions# 1. Preheat &amp; prep# Preheat your oven to 425 \xB0F (220 \xB0C).
`,content:`Stoney made this during his night to cook during the OBX trip in 2025. Ripe bursted cherry tomatoes, stir in burrata or stracciatella, and scoop it all up with crispy garlic crostinis.
Ingredients# # For the dip 2 cups cherry or grape tomatoes 2 Tbsp olive oil (or enough to coat the baking dish) \xBD tsp salt (or to taste) \xBC tsp freshly ground black pepper Pinch of red pepper flakes (optional, for heat) 1\u20132 cloves garlic, minced 1 Tbsp chopped fresh parsley 1 tsp dried oregano (or 1 Tbsp fresh, chopped) 1 burrata (or equivalent amount of stracciatella / burrata filling) Handful of fresh basil leaves, torn 2 Tbsp freshly grated Parmesan (more for serving, optional) # For the garlic crostinis 1 French baguette (or similar crusty bread) 2\u20133 Tbsp butter, softened 1 clove garlic, minced Pinch of salt 1 Tbsp chopped parsley 1 Tbsp grated Parmesan (optional) Instructions# 1. Preheat &amp; prep# Preheat your oven to 425 \xB0F (220 \xB0C).
2. Roast the tomatoes# Place the cherry tomatoes in a baking dish. Drizzle with olive oil, ensuring the bottom is coated. Sprinkle with salt, pepper, red pepper flakes (if using), minced garlic, parsley, and oregano. Toss gently to coat everything evenly. Bake for 20\u201330 minutes, until the tomato skins burst. If some hold their shape, gently press them with the back of a spoon to help them burst. 3. Make the crostinis# While tomatoes are roasting, slice the baguette into ~\xBC- to \xBD-inch slices. In a small bowl, mix softened butter, minced garlic, a pinch of salt, parsley, and Parmesan (if using). Spread this garlic butter mixture onto each bread slice. Arrange the slices on a baking sheet and toast for 5\u20137 minutes, or until golden brown and crunchy. You can do this simultaneously with the tomatoes if your oven has space. 4. Assemble and serve# When the tomatoes are done, remove the baking dish from the oven. Immediately top with torn basil, grated Parmesan, and the burrata (or burrata filling). Gently stir everything so the warm tomatoes mingle with the creamy cheese. Transfer to a serving bowl or keep in the baking dish. Serve hot, alongside the garlic crostinis. Tips &amp; notes# Use very ripe tomatoes\u2014they&rsquo;ll burst more easily and release more flavor. If your burrata is whole, you can cut it open and stir in the creamy center, or just place the whole ball on top and break it when serving. For extra garlic flavor, rub a toasted crostini with a raw cut garlic clove. For more heat, increase red pepper flakes or drizzle with spicy olive oil just before serving. `}).add({id:3,href:"/blog/open-webui/",title:"Using open-webui as a local ChatGPT replacement",description:`Using open-webui# I set up a local LLM using ROCm on my desktop in my last post. So I have the LLM running locally, but its pretty clunky and as a paying user of ChatGPT, I want that cleaner UI/UX. Enter open-webui. This is exactly what I was looking for as a front-end for the ollama server I set up.
I decided to go with the Docker setup.
`,content:`Using open-webui# I set up a local LLM using ROCm on my desktop in my last post. So I have the LLM running locally, but its pretty clunky and as a paying user of ChatGPT, I want that cleaner UI/UX. Enter open-webui. This is exactly what I was looking for as a front-end for the ollama server I set up.
I decided to go with the Docker setup.
Since I fairly recently refreshed my install for Pop, I realized I didn&rsquo;t have Docker installed on my machine anymore. Setting up Docker on Pop is straightforward:
Install Docker# # Add Docker&#39;s official GPG key: sudo apt-get update sudo apt-get install ca-certificates curl sudo install -m 0755 -d /etc/apt/keyrings sudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc sudo chmod a+r /etc/apt/keyrings/docker.asc # Add the repository to Apt sources: echo \\ &#34;deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \\ $(. /etc/os-release &amp;&amp; echo &#34;$VERSION_CODENAME&#34;) stable&#34; | \\ sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null sudo apt-get update sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin You can check that Docker is installed by running docker --version. I prefer Docker Compose when I run containers.
Docker Compose Configuration# This is what my compose file looks like:
version: &#39;3.8&#39; # Specify the version of Docker Compose services: open-webui: image: ghcr.io/open-webui/open-webui:main container_name: open-webui environment: - OLLAMA_BASE_URL=http://127.0.0.1:11434 volumes: - /location/on/host/tosavedata:/app/backend/data network_mode: host restart: always At this point, when you go to localhost:8080 you will see the web front end.
The Interface# The service makes you create an account. Its all local. Nothing is sent anywhere. There is a flag you can set when you spin up your docker container, but I decided to leave it this way since I may eventually set this up so my wife can use it too.
The interface is very familiar if you&rsquo;ve used ChatGPT before. Up at the top you can select your model to use. Its worth setting a default. It doesn&rsquo;t make you set a default, but it gets pretty annoying to open a new chat and have to manually pick a model each time. In my opinion, they should have it select a default on first use, but its not a huge deal at the end of the day.
And that&rsquo;s really it. Now you have a ChatGPT like UX that lets you quickly switch between models, remembers your chats if you need to go back and reference them, and its all running locally.
I plan on running LM Studio as well. Its a little more conducive to &lsquo;playing&rsquo; with the models and prompts and it gives easier access to a lot of the uncensored models that don&rsquo;t have the nanny protections built in. I also plan on setting up a RAG (Retrieval Augmented Generation)with some of my own data to see how useful I can make all of this.
`}).add({id:4,href:"/blog/rocm-on-pop/",title:"Running an LLM locally on Pop!_OS with ROCm support",description:`Running ROCm on Pop!# This has gotten sooo much easier than when I tried to set this up 2 years ago. ROCm has come a long way, but the support and tooling has advanced as well. Now you don&rsquo;t have to jump through tons of hoops to get AI libraries and software to work with ROCm. The last time I tried this, I needed to add ubuntu repos, edit my /etc/os-release file to pretend I was using ubuntu&quot;, and do a rain-dance to get my Raedeon 6900xt in a usable state.
`,content:`Running ROCm on Pop!# This has gotten sooo much easier than when I tried to set this up 2 years ago. ROCm has come a long way, but the support and tooling has advanced as well. Now you don&rsquo;t have to jump through tons of hoops to get AI libraries and software to work with ROCm. The last time I tried this, I needed to add ubuntu repos, edit my /etc/os-release file to pretend I was using ubuntu&quot;, and do a rain-dance to get my Raedeon 6900xt in a usable state.
Then once that was done, you realize that none of the software and libraries incorporated ROCm support and get really sad that you sold your Nvidia GFX card because &ldquo;radeon works better on linux&rdquo;. Basically, 2 years ago, getting ROCm setup was a pain in the ass and your only reward was buggy or nonfunctioning software since the AI/ML world is built on CUDA. Now it&rsquo;s much better with ROCm supporting CUDA and/or most of the libraries incorporating ROCm. Things like pytorch now support AMD ROCm natively.
Getting ROCm installed on Pop!_OS has been simplified to copying and pasting some commands:# wget https://repo.radeon.com/rocm/rocm.gpg.key -O - | gpg --dearmor | sudo tee /etc/apt/keyrings/rocm.gpg &gt; /dev/null echo &#34;deb [arch=amd64 signed-by=/etc/apt/keyrings/rocm.gpg] https://repo.radeon.com/rocm/apt/6.1 jammy main&#34; | sudo tee --append /etc/apt/sources.list.d/rocm.list echo -e &#39;Package: *\\nPin: release o=repo.radeon.com\\nPin-Priority: 600&#39; | sudo tee /etc/apt/preferences.d/rocm-pin-600 Then update and install:
sudo apt update &amp;&amp; sudo apt install rocm Add user to the &lsquo;render&rsquo; group so your user profile has permission to use it.
sudo usermod -a -G render $USER You can confirm it all works with:
rocminfo Installing the actual language models:# Since we have ROCm installed and working properly, let&rsquo;s install a local LLM. I decided to use Ollama, which is Meta&rsquo;s open source model. Ollama.ai actually has a script that helps get it all up-and-running quickly. They even have a Linux version.
I hate that this has become a common practice in the *nix community, because it&rsquo;s such a huge security risk, but they offer a script you can run if you copy and paste this command:
curl -fsSL https://ollama.com/install.sh | sh Only do this if you have reviewed the script and/or can REALLY trust the source. You are downloading a script from the internet and piping it into your shell to run.
There are manual steps here, though, that you can follow if you don&rsquo;t want to use the script: https://github.com/ollama/ollama/blob/main/docs/linux.md
I ran the script (after I reviewed it!). If you&rsquo;ve done everything properly, you should be able to type:
ollama run llama3 The first time you run this it will download llama3, which can take a bit, but it will drop you into a prompt. Where you can talk to it like you would ChatGPT or any other LLM.
And thats that! A working LLM on Pop!_OS using ROCm on your Radeon GFX card! The next step is adding a more user friendly, ChatGPT-like interface for it. This way you can discover and switch models easily, save your history, and easily set system prompts and your own guardrails&ndash;all locally, and privately. That will be on the next post when I have some time to tinker with it.
`}).add({id:5,href:"/recipes/stuffed-peppers/",title:"Stuffed Peppers",description:`These stuffed peppers are filled with a hearty mix of rice, ground meat, and tomato, then baked until the peppers are tender and everything comes together in one simple dish.
`,content:`These stuffed peppers are filled with a hearty mix of rice, ground meat, and tomato, then baked until the peppers are tender and everything comes together in one simple dish.
Ingredients# 3 bell peppers 2 Tbsp cooking oil, divided 1 lb Italian sausage 1 yellow onion, diced 3 garlic cloves, minced 1 tsp Italian seasoning 1/2 tsp garlic powder 1 1/4 tsp salt, divided 1/4 tsp freshly cracked black pepper 1 cup marinara sauce 1/2 cup uncooked long grain white rice 3/4 cup chicken broth 1 cup shredded mozzarella Instructions# Preheat the oven to 350\xB0F. Wash and dry each bell pepper, then cut the bell peppers in half horizontally. Make sure to cut them as evenly as possible. Using a sharp paring knife carefully cut and remove the stem from the top half of each bell pepper (see picture below). It&rsquo;s okay if there is a small hole left where the stem was removed. Prepare the bell peppers. Place each bell pepper half in a 9\xD713-inch casserole dish. Brush the bell peppers with 1 Tbsp oil and season with \xBC tsp of salt and \xBC tsp cracked black pepper. Bake the bell peppers in a preheated oven for 20 minutes to soften. After 20 minutes remove the bell peppers from the oven and set aside. Make the filling. While the bell peppers are baking, heat a large skillet over medium heat and add 1 Tbsp of oil. Brown the Italian sausage. Add the vegetables. Once the sausage has browned, add the diced onion and minced garlic to the skillet. Continue to saut\xE9 over medium heat until the onion is translucent and the garlic is fragrant. Combine remaining ingredients. Next add the uncooked rice, marinara sauce, Italian seasoning, garlic powder, 1 tsp of salt, and chicken broth to the skillet. Stir to combine. Cook the filling. Place a lid on the skillet, turn the heat up to medium-high, and allow the mixture to come to a full boil. Once boiling, immediately reduce the heat to medium-low and allow the mixture to simmer, without lifting the lid or stirring, for 20 minutes. After 20 minutes, turn the heat off and let it rest, without lifting the lid, for an additional 5 minutes. Fill the bell peppers. Next remove the lid, fluff the rice, and stir the mixture again to redistribute the ingredients. Begin to fill each bell pepper with the meat filling. Stuff as much filling as you can into each, filling them all the way to the top. Add cheese and bake. Top each bell pepper evenly with shredded mozzarella cheese. Loosely place some tented aluminum foil over the top of the casserole dish and bake for 15 minutes. After 15 minutes the bell peppers should be tender but not mushy. Broil the bell peppers. Now remove the foil and turn the heat on to broil. Broil the stuffed bell peppers for 2-3 minutes or just until the cheese gets a little brown on top. Be sure to watch the bell peppers closely at this step to prevent the cheese from over browning. `}).add({id:6,href:"/blog/migration-pt2/",title:"Migration Pt2",description:`Updating DNS Records# This was a fairly straightforward process:
Log into GitHub &gt; Go to your public repo &gt; Settings &gt; Pages: and enter your domain in the Custom Domain section Add your domain name www.wesgould.com Log into your DNS provider. Delete CNAME / A-RECORDs for www.wesgould.com and wesgould.com to avoid conflicts with the new records. ADD CNAME RECORD for www.wesgould.com. Add A-RECORD for wesgould.com. To create A records, point your apex domain to the IP addresses for GitHub Pages: - 185.199.108.153 - 185.199.109.153 - 185.199.110.153 - 185.199.111.153 `,content:`Updating DNS Records# This was a fairly straightforward process:
Log into GitHub &gt; Go to your public repo &gt; Settings &gt; Pages: and enter your domain in the Custom Domain section Add your domain name www.wesgould.com Log into your DNS provider. Delete CNAME / A-RECORDs for www.wesgould.com and wesgould.com to avoid conflicts with the new records. ADD CNAME RECORD for www.wesgould.com. Add A-RECORD for wesgould.com. To create A records, point your apex domain to the IP addresses for GitHub Pages: - 185.199.108.153 - 185.199.109.153 - 185.199.110.153 - 185.199.111.153 To create AAAA records, point your apex domain to the IP addresses for GitHub Pages: - 2606:50c0:8000::153 - 2606:50c0:8001::153 - 2606:50c0:8002::153 - 2606:50c0:8003::153 Be impatient because &ldquo;it didn&rsquo;t work!&rdquo; (immediately). But it&rsquo;s DNS propagation and it&rsquo;s slow. I understand why it takes a while, and theoretically it could take days to fully propagate, but it also seems crazy that it&rsquo;s not instantaneous in 2023. That said, it probably took me longer to complain here than it did to propagate. Try accessing the site. GitHub may report an error at first under where you entered your custom domain, but check again, and it should work. wesgould.com is improperly configured Domain does not resolve to the GitHub Pages server. For more information, see documentation (NotServedByPagesError). Make sure you check the HTTPS box. Do one last terminal dig to see the changes like a nerd. Bug your wife and friends to see if they can get to your site. Pretend they are as excited as you are even though &ldquo;Okay now what&rdquo; is about all you&rsquo;ll get from them. That&rsquo;s an exact quote from my wife. `}).add({id:7,href:"/recipes/homebakes/",title:"Homebakes",description:`Home Bakes# Family recipe from Oma passed down.
Ingredients# 2 cups Sugar 4 Tbls. Cocoa 1 stick Butter \xBD cup Milk \xBD cup Peanut Butter 2 \xBD cups 1 Minute Oatmeal Directions:# 1. Lay down two strips of waxed paper. (Foil can also be used in a pinch.)
2. In a large saucepan, combine sugar and cocoa. Stir until well blended.
`,content:`Home Bakes# Family recipe from Oma passed down.
Ingredients# 2 cups Sugar 4 Tbls. Cocoa 1 stick Butter \xBD cup Milk \xBD cup Peanut Butter 2 \xBD cups 1 Minute Oatmeal Directions:# 1. Lay down two strips of waxed paper. (Foil can also be used in a pinch.)
2. In a large saucepan, combine sugar and cocoa. Stir until well blended.
3. Add milk and stir. Then add a stick of butter.
4. Place over medium heat and cook until the mixture comes to a complete rolling boil, stirring often. Boil for 2 minutes (use a timer or watch the clock).
5. Remove from the heat source.
6. Add peanut butter and stir until it is all melted.
7. Add oats and stir.
8. Using a tablespoon, immediately spoon the mixture onto wax paper, working quickly.
9. Allow the cookies to cool; they will set as they cool.
`}).add({id:8,href:"/blog/migrating-to-hugo/",title:"Migrating to Hugo",description:`It&rsquo;s been an&hellip;adventure.
Why Switch from Publii to Hugo?# I am switching from Publii to Hugo because I wanted to take advantage of the text notes I was already taking with Vimwiki for my various projects. The Publii interface was nice, but I&rsquo;m already writing notes in a format that doesn&rsquo;t require me to use their WYSIWYG editor. I know that seems lazy, but it&rsquo;s an extra step that caused just enough friction that I wouldn&rsquo;t update the blog.
`,content:`It&rsquo;s been an&hellip;adventure.
Why Switch from Publii to Hugo?# I am switching from Publii to Hugo because I wanted to take advantage of the text notes I was already taking with Vimwiki for my various projects. The Publii interface was nice, but I&rsquo;m already writing notes in a format that doesn&rsquo;t require me to use their WYSIWYG editor. I know that seems lazy, but it&rsquo;s an extra step that caused just enough friction that I wouldn&rsquo;t update the blog.
I picked Hugo because it seemed simple, used text files vs. databases, etc., and honestly, I found a sick Gruvbox based theme that I liked. Gruvbox is the one true color palette.
Working through the kinks# Setting it up was actually really easy. I got it all working locally in about an hour, but when I went to deploy it to the S3 bucket, it just didn&rsquo;t work. I discovered that AWS S3 buckets don&rsquo;t like &ldquo;pretty URLs&rdquo;. In other words, S3 buckets need the URL with the &ldquo;.html&rdquo; at the end, or it throws an error.
The fix is using &ldquo;ugly URLs&rdquo;. This argument, added to the Hugo config.toml, should add the .html to the end of all of the URLs. It does that, but it also provides the ENTIRE URL. So, if you clicked the link to the &lsquo;blog&rsquo;, instead of going to wesgould.com/blog.html, it would go to wesgould.com/www.wesgould.com/blog.html. So either Hugo has a bug in it, or the way my particular theme is set up, that ALSO didn&rsquo;t work. After fighting it for entirely too long, I decided that I didn&rsquo;t want to recode the entire theme. I&rsquo;d just move from AWS to GitHub pages&ndash;which does support prettyURLs.
It seemed simple and would save me $0.50 a month (since getting TLS certs requires CloudFront, which costs the exorbitant $0.50/mo). It also aligned with the Git workflow I was already using.
I read a couple of tutorials but ultimately ended up following this amazing YouTube video. It walks you through creating a &ldquo;working code&rdquo; repository and a public repository where you can publish your static site content.
Hooray! This will be easy peasy!
But, I ran into another problem and beat my head against the wall. The site deployed and seemed to work fine, except for the Blog URL. It was directing me to a weird readme. The source of the page referenced Jekyll and SEO optimizations. I grepped for those strings in my code and couldn&rsquo;t find them. Long story short, the culprit was a &ldquo;gh-pages&rdquo; branch of code in my website repo. Having a gh-pages branch (designed for Jekyll) in your repo tells GitHub to kick off an automated workflow to deploy a static site. Cool.
I&rsquo;d started to follow this tutorial before I found that YouTube video. So, I had created a gh-pages branch and totally forgot about it. Apparently, that magical branch automatically injects stuff into your code/site as it&rsquo;s updated\u2014and in my case, broke the &ldquo;blog&rdquo; links. Deleting that branch fixed all of the problems.
It&rsquo;s working now, and I am looking forward to tuning it. The only thing I don&rsquo;t like is that it, probably correctly, follows the system setting for light vs. dark mode, even though I&rsquo;ve set the default to dark. Light mode Gruvbox is gross. I might try to fix it and submit a PR to the theme editor. I&rsquo;ve seen other themes that let users choose the system, light, or dark.
The last thing I need to do is clean up DNS so my actual domain displays this instead of my broken S3 version.
`}).add({id:9,href:"/blog/creating-nfs-shares/",title:"Creating NFS Shares",description:`Creating an NFS share# If you are trying to share between *nix systems, the Network File Servers (NFS) are really easy to set up. Note: If you are planning on sharing with Windows machines, SMB/Samba is a better option.
It should also be noted that NFS doesn&rsquo;t restrict, by default, to individual users. It limits to IPs and IP ranges&ndash;which makes it ideal for secure networks, but less-than-ideal for non-secure / public networks.
`,content:`Creating an NFS share# If you are trying to share between *nix systems, the Network File Servers (NFS) are really easy to set up. Note: If you are planning on sharing with Windows machines, SMB/Samba is a better option.
It should also be noted that NFS doesn&rsquo;t restrict, by default, to individual users. It limits to IPs and IP ranges&ndash;which makes it ideal for secure networks, but less-than-ideal for non-secure / public networks.
It&rsquo;s pretty straight forward (on debian/apt bases distributions):
Installing the NFS server# sudo apt-get update &amp;&amp; sudo apt-get install nfs-kernel-server Make the directory you would like to share. This directory can go anywhere but I think standard location is in the /mnt/ directory.
sudo mkdir -p /mnt/your-directory-to-share This (below) changes the permissions to let all of the client machines access to the directory. You can change the permissions as well but for now I left it
sudo chown -R nobody:nogroup /mnt/your-directory-to-share\` Give the clients read,write, and execute permissions as well:
sudo chmod -R 777 /mnt/your-directory-to-share\` At this point your nfs-server is installed (not active) and your shared folder is setup. Now you need to grant access to the NFS share. To do that you have to edit the &rsquo;exports&rsquo; file&ndash;which is found at /etc/exports.
sudo vim /etc/exports /mnt/your-directory-to-share &lt;ipaddress of the client&gt;(rw,sync,no_subtree_check) If you want to add specific IPs you have to create a new line for each of them. However, if you wanted to do an entire subnet/ip range you can write something like this: 192.168.1.0/24 (which would share 192.168.1.1-255)
rw = read/write ; sync = means changes have to be written to disk before they are applied ; no_subtree_check = means what it says. Apparently, subtree checking causes more problems than its worth, so most people recommend using this flag to turn it off.
According the NFS man pages and tutorials I found you are supposed to run the following in this order:
sudo exportfs -a sudo systemctl restart nfs-kernel-server This threw an error for me. I ran them a second time and it seems to work. I think you have to have the nfs-kernel-server running before you can do sudo exportfs -a. When you run the restart command it actually starts it for the first time, then you can follow the steps properly.
If it was up to me, I think it should be:
sudo systemctl start nfs-kernel-server sudo exportfs -a sudo systemctl restart nfs-kernel-server Connect your client# So now you have the server serving up the shared drive. Now you need to connect to it.
On your laptop/pc/whatever is connecting to the NFS server you need to install the NFS client and mount the drive.
sudo apt-get install nfs-common Make a directory/mount point for this. Again, this can be anywhere. If you wanted it to be in your home folder it can be, but we&rsquo;ll stick with /mnt/nfsshare
sudo mkdir -p /mnt/nfsshare Then you mount it:
sudo mount &lt;ip-of-NFS-server&gt;:/mnt/your-directory-to-share /mnt/nfsshare And thats it. When you navigate to /mnt/nfsshare on your client machine you will see all of the contents of the nfs drive. Which at this point would be empty.
If you want to mount it permanently, just edit your /etc/fstab file by adding a line at the end:
ip-of-NFS-server&gt;:/mnt/your-directory-to-share /mnt/nfsshare nfs defaults 0 0 This article was updated on October 17, 2021
`}).add({id:10,href:"/recipes/enchilada-casserole/",title:"Enchilada Casserole",description:`One of Mom\u2019s favorite recipes from Kris H. &amp; Robin K.
Ingredients# 2 cups grated cheddar cheese 1/2 cup finely chopped green onion 2 cans cream of chicken soup 1 cup sour cream 1 can chopped green chilies 1/2 tsp salt 2 cans chicken (all white meat) OR 4 chicken breasts, cooked and chopped 12 corn tortillas, hand shredded Important Ingredient Note Use Chopped Green Chilies from the Mexican Food section - these come in very small cans. DO NOT GET FIRE-ROASTED versions! The cans look almost identical but the taste is completely different. Chicken Cooking Tip Cook raw chicken breasts by placing in a sauce pan, just cover with water, and bring to boil. Turn heat to low and simmer for 30 minutes to 1 hour until no pink remains in center. Remove chicken, cool, and chop into bite-sized pieces. This keeps the meat tender and moist. Instructions# Preheat oven to 350\xB0F.
`,content:`One of Mom\u2019s favorite recipes from Kris H. &amp; Robin K.
Ingredients# 2 cups grated cheddar cheese 1/2 cup finely chopped green onion 2 cans cream of chicken soup 1 cup sour cream 1 can chopped green chilies 1/2 tsp salt 2 cans chicken (all white meat) OR 4 chicken breasts, cooked and chopped 12 corn tortillas, hand shredded Important Ingredient Note Use Chopped Green Chilies from the Mexican Food section - these come in very small cans. DO NOT GET FIRE-ROASTED versions! The cans look almost identical but the taste is completely different. Chicken Cooking Tip Cook raw chicken breasts by placing in a sauce pan, just cover with water, and bring to boil. Turn heat to low and simmer for 30 minutes to 1 hour until no pink remains in center. Remove chicken, cool, and chop into bite-sized pieces. This keeps the meat tender and moist. Instructions# Preheat oven to 350\xB0F.
Prepare the filling. Mix in a large sauce pan the chopped cooked chicken breasts, cream of chicken soup, sour cream, and green chilies. Stir until blended over low heat.
Combine ingredients. Remove from heat and add cheddar cheese, green onion, and tortillas. Add salt and pepper to taste.
Assemble casserole. Place all ingredients in a 9&quot; x 13&quot; casserole dish and top lightly with grated cheese.
Bake the casserole. Bake for 20 to 30 minutes at 350\xB0F until bubbling.
Serving Suggestion This casserole is delicious on its own or served with Tostitos for extra crunch! `}).add({id:11,href:"/recipes/cincinatti-chili/",title:"Cincinatti Chili",description:`Ingredients# # For the chili 2 pounds ground beef (80:20 is good; 90:10 works as well and will obviously be less fatty) 1 6-ounce can tomato paste 4 cups water 1 (8-ounce) can tomato sauce 1 large onion, minced 6 cloves garlic, minced (pre-minced in the jar is fine) 3 tablespoons chili powder 1 teaspoon cumin 1 teaspoon cinnamon 3/4 teaspoon ground allspice 1/4 teaspoon ground cloves 1/2 teaspoon cayenne 2 teaspoons kosher salt 2 tablespoons Worcestershire sauce TIP: Measure all of your ingredients first. I usually set all of the spices on my left with a small bowl in front of me. As I measure and put each ingredient into the bowl, I move the spice bottle to the right side so I know I&rsquo;ve added it. It&rsquo;s really easy to forget which spices you&rsquo;ve measured out!
`,content:`Ingredients# # For the chili 2 pounds ground beef (80:20 is good; 90:10 works as well and will obviously be less fatty) 1 6-ounce can tomato paste 4 cups water 1 (8-ounce) can tomato sauce 1 large onion, minced 6 cloves garlic, minced (pre-minced in the jar is fine) 3 tablespoons chili powder 1 teaspoon cumin 1 teaspoon cinnamon 3/4 teaspoon ground allspice 1/4 teaspoon ground cloves 1/2 teaspoon cayenne 2 teaspoons kosher salt 2 tablespoons Worcestershire sauce TIP: Measure all of your ingredients first. I usually set all of the spices on my left with a small bowl in front of me. As I measure and put each ingredient into the bowl, I move the spice bottle to the right side so I know I&rsquo;ve added it. It&rsquo;s really easy to forget which spices you&rsquo;ve measured out!
Directions# Cook the tomato paste: Heat a large, heavy-bottomed pot or Dutch oven over medium-high heat. Add the tomato paste to the dry pot and cook, constantly scraping the bottom with a wooden spoon or silicone spatula, until the tomato smells rich and toasty and you start to see browned (not burned) patches in the bottom of the pot. This should take 1 to 3 minutes.
Combine the ingredients in a pot: Remove the pot from heat and add the ground beef and water. Mix them together into a sludge. It will not look pretty, but press on. There&rsquo;s a method to this madness. Return to medium-high heat and bring to a simmer, stirring all the while, so the sludge breaks up into a mealy paste. Add all the remaining ingredients.
Simmer gently, uncovered, for 2 to 3 hours: Stir the chili often. You want the volume to reduce a bit. If it starts to lose too much water and is getting too thick, reduce the heat and cover with a lid&ndash;leaving just a bit for some steam to escape. It will be ready in an hour, but the longer you let it simmer the richer the flavor will be.
`}).add({id:12,href:"/blog/turkey-trot-2019/",title:"Turkey Trot 2019",description:`Turkey Trot 2k19# Date: November 28, 2019
The family decided to go to a Turkey Trot before we ate Thanksgiving dinner. I bet my niece $1 that I would run it in under 40 minutes. I got &lt; 33 minutes (unofficially\u2014the times aren&rsquo;t posted yet).
About 900 people signed up for this particular 5k!
The 2nd prize was a huge pie (and a $50 Dick&rsquo;s gift card, but you can&rsquo;t eat that so it&rsquo;s less important).
`,content:`Turkey Trot 2k19# Date: November 28, 2019
The family decided to go to a Turkey Trot before we ate Thanksgiving dinner. I bet my niece $1 that I would run it in under 40 minutes. I got &lt; 33 minutes (unofficially\u2014the times aren&rsquo;t posted yet).
About 900 people signed up for this particular 5k!
The 2nd prize was a huge pie (and a $50 Dick&rsquo;s gift card, but you can&rsquo;t eat that so it&rsquo;s less important).
Disclaimer: This post was narrated by my chief editor\u2014 my niece.
This article was updated on August 12, 2020
`}),X.addEventListener("input",function(){let n=this.value,i=e.search(n,5,{enrich:!0}),s=new Map;for(let r of i.flatMap(l=>l.result))s.has(r.href)||s.set(r.doc.href,r.doc);if(z.innerHTML="",z.classList.remove("search__suggestions--hidden"),s.size===0&&n){let r=document.createElement("div");r.innerHTML=`No results for "<strong>${n}</strong>"`,r.classList.add("search__no-results"),z.appendChild(r);return}for(let[r,l]of s){let h=document.createElement("a");h.href=r,h.classList.add("search__suggestion-item"),z.appendChild(h);let p=document.createElement("div");p.textContent=l.title,p.classList.add("search__suggestion-title"),h.appendChild(p);let f=document.createElement("div");if(f.textContent=l.description,f.classList.add("search__suggestion-description"),h.appendChild(f),z.childElementCount===5)break}})})();})();
//! Source: https://github.com/h-enk/doks/blob/master/assets/js/index.js
/*! Source: https://dev.to/shubhamprakash/trap-focus-using-javascript-6a3 */
//! Source: https://discourse.gohugo.io/t/range-length-or-last-element/3803/2
